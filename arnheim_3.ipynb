{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spHtkST4105t"
      },
      "source": [
        "Copyright 2021 DeepMind Technologies Limited\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfSc66QYgnfR"
      },
      "source": [
        "# Arnheim 3 - Collage\n",
        "\n",
        "**Piotr Mirowski, Dylan Banarse, Mateusz Malinowski, Yotam Doron, Oriol Vinyals, Simon Osindero, Chrisantha Fernando**\n",
        "\n",
        "DeepMind, 2021\n",
        "\n",
        "![picture](https://github.com/deepmind/arnheim/raw/main/images/arnheim3_examples.png)\n",
        "Clockwise from top left: \"Sri Lankan objects\" (200 transparent patches); \"Waves\" (70 masked transparnecy patches with background); \"Fruit bowl\" (100 opacity patches); \"Fruit bowl\" (100 transparent patches); \"Face\" (7 opacity patches); \"Swans\" (100 masked transparent patches); \"Chicken\" (70 masked transparent patches); \"Dancer\" (40 transparent patches). See description in the [videos](https://www.youtube.com/watch?v=HKDQsrO5xF4&list=PLKhLdFXp1JN5SEV56w9OWWsT5pAz9z7G_) for settings.\n",
        "\n",
        "**STEPS:**\n",
        "\n",
        "Quickstart:\n",
        "1. Click \"Connect\" in the top right corner\n",
        "1. Runtime -> Run all\n",
        "\n",
        "Play around:\n",
        "* Parameters section to configure collage creation settings\n",
        "* Image patches section to upload your own patches\n",
        "* Make Drawing section to set the text prompt\n",
        "\n",
        "**An Exploration of Architectures and Losses for Painting and Drawing**\n",
        "\n",
        "Arnheim 3 is an algorithm which generates collages by training by gradient descent a network which applies affine transformations, i.e translation, scaling, rotation, and shear to a set of image patches, this set being subject to evolution in the outer loop. \n",
        "\n",
        "The signal for how good an image is comes from CLIP, a text-image dual encoder. This work simplifies and extends Arnheim 2 which also used CLIP but generated SVG strokes using a more complex hierarchical stroke grammar. \n",
        "\n",
        "Here you can experiment with a variety of rendering methods for combining patches in a learnable way.\n",
        "\n",
        "**New Features**\n",
        "1. Tiling\n",
        "\n",
        "  Multiple images can now be tiled to create arbitrary large images. The individual images (referred to as *tiles*) are drawn sequentially starting at the top left. All the tiles overlap each other so the drawing process can blend content of neighbouring tiles. \n",
        "\n",
        "1. Compositional Images\n",
        "\n",
        "  Uses 3x3 prompts covering over-lapping regions of the image to specify different content across the whole image. The main prompt guides the direction of overall image.\n",
        "\n",
        "1. Coloured Background\n",
        "\n",
        "  User-selectable background colour or use of uploaded images.\n",
        "\n",
        "1. Interactive Patch Placement\n",
        "\n",
        "  Stop the \"Create collage! (Loop)\" cell at any time and run the \"Tinker with patches\" cell below it to adjust individual patches with sliders. Then re-run the \"Create collage! (Loop)\" cell to continue generation.\n",
        "\n",
        "**Tips**\n",
        "\n",
        "**Tiling** produces hard edges if patches go outside the tile canvas. To alleviate this restrict the patch traslation and keep them relatively small, e.g. using these settings:\n",
        "\n",
        "* MIN_TRANS=-0.66\n",
        "* MAX_TRANS=0.8\n",
        "* PATCH_MAX_PROPORTION=5\n",
        "* FIXED_PATCH_SCALE=OFF\n",
        "\n",
        "**opacity rendering** uses alpha and depth to render semi-opaque overlapping patches which allow gradients to be used during learning. The translucency is reduced over the course of learning to end with opaque patches. When using a small number of patches evolution can perform better than learning alone. For example, with only 7 patches a population of 10 with the Evolutionary Strategies method applied at every step can yield good results. The settings to get the face image above were:\n",
        "\n",
        "* Prompt “Face”\n",
        "* 7 patches\n",
        "* opacity\n",
        "* 400 steps\n",
        "* ES evolution every step\n",
        "* POP_SIZE=10\n",
        "* ROT_POS_MUTATION=0.05\n",
        "* SCALE_MUTATION = 0.02\n",
        "* PATCH_MUTATION = 0.2\n",
        "\n",
        "**Transparency rendering** works well as gradients are more effective. Note that colours are additive so setting INITIAL_MIN_RGB=0.1 and INITIAL_MAX_RGB=0.5 helps reduce bleaching. Something to try is:\n",
        "\n",
        "* 80 Patches\n",
        "* Transparency\n",
        "* INITIAL_MIN_RGB=0.1; INITIAL_MAX_RGB=0.5\n",
        "* 15000 steps\n",
        "* Microbial GA every 100 steps\n",
        "* POP_SIZE = 2\n",
        "* LEARNING_RATE = 0.07\n",
        "* PATCH_MUTATION_PROBABILITY = 1\n",
        "* Prompt \"Swans on a pond\"\n",
        "\n",
        "**Masked Transparency rendering** also works well as gradients are more effective.  Something to try is:\n",
        "\n",
        "* 200 Patches\n",
        "* MASKED Transparency\n",
        "* INITIAL_MIN_RGB=0.7; INITIAL_MAX_RGB=1.0\n",
        "* 15000 steps\n",
        "* Microbial GA every 100 steps\n",
        "* POP_SIZE = 2\n",
        "* LEARNING_RATE = 0.07\n",
        "* PATCH_MUTATION_PROBABILITY = 1\n",
        "* Prompt \"Swans on a pond\"\n",
        "\n",
        "**Note that the Colab can easily run out of memory with large populations, many patches and large patch sizes! If you start to encounter CUDA memory issues try lowering the number of patches and restarting the Colab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfEKdIbK4VZa"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5dyyH781qzIC"
      },
      "outputs": [],
      "source": [
        "#@title Installation of libraries {vertical-output: true}\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "  torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "  torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "  torch_version_suffix = \"\"\n",
        "else:\n",
        "  torch_version_suffix = \"+cu110\"\n",
        "\n",
        "%cd /content/\n",
        "!pip install cssutils\n",
        "!pip install torch-tools\n",
        "!pip install   visdom\n",
        "!pip install kornia\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git --no-deps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8bdUyJs4hq3"
      },
      "source": [
        "# Imports and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGB-vb1MIXDd"
      },
      "outputs": [],
      "source": [
        "#@title Imports {vertical-output: true}\n",
        "import clip\n",
        "import copy\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "import io\n",
        "from kornia.color import hsv\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import moviepy.editor as mvp\n",
        "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import requests\n",
        "from skimage.transform import resize\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "os.environ[\"FFMPEG_BINARY\"] = \"ffmpeg\"\n",
        "print(\"Torch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yIVTTQO-lLCx"
      },
      "outputs": [],
      "source": [
        "#@title Initialise and load CLIP model {vertical-output: true}\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "CLIP_MODEL = \"ViT-B/32\"\n",
        "print(f\"Downloading CLIP model {CLIP_MODEL}...\")\n",
        "clip_model, _ = clip.load(CLIP_MODEL, device, jit=False)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlVNTb_1NelG"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j70Ff0qRNBjh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Collage configuration\n",
        "#@markdown Render methods\n",
        "\n",
        "#@markdown \n",
        "#@markdown **opacity** patches overlay each other using a combination of alpha and depth,\n",
        "#@markdown **transparency** _adds_ patch colours (black therefore appearing transparent),\n",
        "#@markdown **masked transparency normed** blends patches using a normalised alpha channel where areas of maximum patch overlap are opaque and all other areas are translucent.\n",
        "#@markdown and **masked transparency clipped** blends patches using a clipped alpha channel where all regions with alpha > 1 are opaque.\n",
        "RENDER_METHOD = \"masked_transparency_normed\"  #@param [\"opacity\", \"transparency\", \"masked_transparency_normed\", \"masked_transparency_clipped\"]\n",
        "NUM_PATCHES =      150#@param {type:\"integer\"}\n",
        "COLOUR_TRANSFORMATIONS = \"RGB space\"  #@param [\"none\", \"RGB space\", \"HSV space\"]\n",
        "#@markdown Invert image colours to have a white background?\n",
        "INVERT_COLOURS = False #@param {type:\"boolean\"}\n",
        "\n",
        "CANVAS_WIDTH = 224\n",
        "CANVAS_HEIGHT = 224\n",
        "MULTIPLIER_BIG_IMAGE = 4\n",
        "\n",
        "#@markdown Use additional prompts for different regions\n",
        "COMPOSITIONAL_IMAGE = False #@param {type:\"boolean\"}\n",
        "if COMPOSITIONAL_IMAGE:\n",
        "  CANVAS_WIDTH *= 2\n",
        "  CANVAS_HEIGHT *= 2\n",
        "  MULTIPLIER_BIG_IMAGE = int(MULTIPLIER_BIG_IMAGE / 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5vRneu99YwV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Affine transform settings\n",
        "\n",
        "#@markdown Translation bounds for X and Y.\n",
        "MIN_TRANS = -0.66  #@param{type:\"slider\", min:-1.0, max:1.0, step:0.01}\n",
        "MAX_TRANS = 0.8  #@param{type:\"slider\", min:-1.0, max:1.0, step:0.01}\n",
        "#@markdown Scale bounds (> 1 means zoom out and < 1 means zoom in).\n",
        "MIN_SCALE =   1#@param {type:\"number\"}\n",
        "MAX_SCALE =   2#@param {type:\"number\"\n",
        "#@markdown Bounds on ratio between X and Y scale (default 1).\n",
        "MIN_SQUEEZE =   0.5#@param {type:\"number\"}\n",
        "MAX_SQUEEZE =   2.0#@param {type:\"number\"}\n",
        "#@markdown Shear deformation bounds (default 0)\n",
        "MIN_SHEAR = -0.2  #@param{type:\"slider\", min:-1.0, max:1.0, step:0.01}\n",
        "MAX_SHEAR = 0.2  #@param{type:\"slider\", min:-1.0, max:1.0, step:0.01}\n",
        "#@markdown Rotation bounds.\n",
        "MIN_ROT_DEG = -180 #@param{type:\"slider\", min:-180, max:180, step:1}\n",
        "MAX_ROT_DEG = 180 #@param{type:\"slider\", min:-180, max:180, step:1}\n",
        "MIN_ROT = MIN_ROT_DEG * np.pi / 180.0\n",
        "MAX_ROT = MAX_ROT_DEG * np.pi / 180.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMn71brOJb1Z",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Colour transform settings\n",
        "\n",
        "#@markdown RGB\n",
        "MIN_RGB = -0.21  #@param {type:\"slider\", min: -1, max: 1, step: 0.01}\n",
        "MAX_RGB = 1.0  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "INITIAL_MIN_RGB = 0.9  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "INITIAL_MAX_RGB = 1  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "#@markdown HSV\n",
        "MIN_HUE = 0.  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "MAX_HUE_DEG = 360 #@param {type:\"slider\", min: 0, max: 360, step: 1}\n",
        "MAX_HUE = MAX_HUE_DEG * np.pi / 180.0\n",
        "MIN_SAT = 0.  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "MAX_SAT = 1.  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "MIN_VAL = 0.  #@param {type:\"slider\", min: -1, max: 1, step: 0.01}\n",
        "MAX_VAL = 1.  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFZ31zE0AAKJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Training settings\n",
        "\n",
        "# Reasonable defaults:\n",
        "# OPTIM_STEP = 10000 to 20000\n",
        "# LEARNING_RATE = 0.1\n",
        "# NUM_AUGS = 2 to 4\n",
        "# GRADIENT_CLIPPING = 10.0\n",
        "# USE_NORMALIZED_CLIP = True\n",
        "\n",
        "#@markdown Number of training steps\n",
        "OPTIM_STEPS = 6000    #@param{type:\"slider\", min:200, max:20000, step:100}\n",
        "\n",
        "LEARNING_RATE = 0.12    #@param{type:\"slider\", min:0.0, max:0.6, step:0.01}\n",
        "#@markdown Number of augmentations to use in evaluation\n",
        "USE_IMAGE_AUGMENTATIONS = True #@param{type:\"boolean\"}\n",
        "NUM_AUGS = 4  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Normalize colours for CLIP, generally leave this as True\n",
        "USE_NORMALIZED_CLIP = False  #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Gradient clipping during optimisation\n",
        "GRADIENT_CLIPPING = 10.0  #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Initial random search size (1 means no search)\n",
        "INITIAL_SEARCH_SIZE = 1 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "if COMPOSITIONAL_IMAGE:\n",
        "  print(\"Using ONE image augmentations for compositional image creation.\")\n",
        "  USE_IMAGE_AUGMENTATIONS = True\n",
        "  NUM_AUGS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-yFYf0vAS42",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Evolution settings\n",
        "\n",
        "# Reasonable defaults:\n",
        "# POP_SIZE = 2\n",
        "# EVOLUTION_FREQUENCY = 100\n",
        "# MUTION SCALES = ~0.1\n",
        "# MAX_MULTIPLE_VISUALISATIONS = 7\n",
        "\n",
        "#@markdown For evolution set POP_SIZE greater than 1\n",
        "POP_SIZE =    2  #@param{type:\"slider\", min:1, max:100}\n",
        "EVOLUTION_FREQUENCY =  100#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Genetic algorithm methods\n",
        "\n",
        "#@markdown **Microbial** - loser of randomly selected pair is replaced by mutated winner. A low selection pressure.\n",
        "\n",
        "#@markdown **Evolutionary Strategies** - mutantions of the best individual replace the rest of the population. Much higher selection pressure than Microbial GA.\n",
        "GA_METHOD = \"Microbial\"  #@param [\"Evolutionary Strategies\", \"Microbial\"]\n",
        "#@markdown ### Mutation levels\n",
        "#@markdown Scale mutation applied to position and rotation, scale, distortion, colour and patch swaps.\n",
        "POS_AND_ROT_MUTATION_SCALE = 0.02  #@param{type:\"slider\", min:0.0, max:0.3, step:0.01}\n",
        "SCALE_MUTATION_SCALE = 0.02  #@param{type:\"slider\", min:0.0, max:0.3, step:0.01}\n",
        "DISTORT_MUTATION_SCALE = 0.02  #@param{type:\"slider\", min:0.0, max:0.3, step:0.01}\n",
        "COLOUR_MUTATION_SCALE = 0.02  #@param{type:\"slider\", min:0.0, max:0.3, step:0.01}\n",
        "PATCH_MUTATION_PROBABILITY = 1  #@param{type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "#@markdown Limit the number of individuals shown during training\n",
        "MAX_MULTIPLE_VISUALISATIONS =   5#@param {type:\"integer\"}\n",
        "#@markdown Save video of population sample over time.\n",
        "POPULATION_VIDEO = True  #@param (type:\"boolean\")\n",
        "\n",
        "USE_EVOLUTION = POP_SIZE > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "37P41H1vGu-0"
      },
      "outputs": [],
      "source": [
        "# @title Saving images on Drive\n",
        "#@markdown Displayed results can also be stored on Google Drive.\n",
        "STORE_ON_GOOGLE_DRIVE = False  #@param {type:\"boolean\"}\n",
        "GOOGLE_DRIVE_RESULTS_DIR = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "MOUNT_DIR = \"/content/drive\"\n",
        "\n",
        "if STORE_ON_GOOGLE_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount(MOUNT_DIR)\n",
        "  DIR_RESULTS = pathlib.PurePath(MOUNT_DIR, \"MyDrive\", GOOGLE_DRIVE_RESULTS_DIR)\n",
        "  print(f\"Storing results on Google Drive in {DIR_RESULTS}\")\n",
        "else:\n",
        "  DIR_RESULTS = \"/content\"\n",
        "  print(f\"Storing results in Colab in {DIR_RESULTS}\")\n",
        "\n",
        "pathlib.Path(DIR_RESULTS).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbGkH7v-_--H"
      },
      "source": [
        "# Images patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ7_ChjTPcnf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Functions used for loading patches\n",
        "\n",
        "def add_binary_alpha_mask(patch):\n",
        "  \"\"\"Black pixels treated as having alpha=0, all other pixels have alpha=255\"\"\"\n",
        "  shape = patch.shape\n",
        "  mask = ((patch.sum(2) > 0) * 255).astype(np.uint8)\n",
        "  return np.concatenate([patch, np.expand_dims(mask, -1)], axis=-1)\n",
        "\n",
        "\n",
        "def resize_patch(patch, coeff):\n",
        "  return resize(patch.astype(float),\n",
        "                (int(np.round(patch.shape[0] * coeff)),\n",
        "                 int(np.round(patch.shape[1] * coeff))))\n",
        "\n",
        "\n",
        "def print_size_segmented_data(segmented_data):\n",
        "  size_max = 0\n",
        "  shape_max = None\n",
        "  size_min = np.infty\n",
        "  shape_min = None\n",
        "  ws = []\n",
        "  hs = []\n",
        "  for i, segment in enumerate(segmented_data):\n",
        "    segment = segment.swapaxes(0, 1) \n",
        "    shape_i = segment.shape\n",
        "    size_i = shape_i[0] * shape_i[1]\n",
        "    if size_i > size_max:\n",
        "      shape_max = shape_i\n",
        "      size_max = size_i\n",
        "    if size_i < size_min:\n",
        "      shape_min = shape_i\n",
        "      size_min = size_i\n",
        "    im_i = cv2.cvtColor(segment, cv2.COLOR_RGBA2BGRA)\n",
        "    im_bgr = im_i[:, :, :3]\n",
        "    im_mask = np.tile(im_i[:, :, 3:], (1, 1, 3))\n",
        "    im_render = np.concatenate([im_bgr, im_mask], 1)\n",
        "    print(f'Patch {i} of shape {shape_i}')\n",
        "    cv2_imshow(im_render)\n",
        "  print(f\"{len(segmented_data)} patches, max {shape_max}, min {shape_min}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXuH-1hcAl2M",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load segmented patches\n",
        "\n",
        "#@markdown Patch files - select example sets or your own:\n",
        "PATCH_SET = \"Animals\" #@param [\"Fruit and veg\", \"Sea glass\", \"Animals\", \"Handwritten MNIST\", \"Upload to Colab\", \"Load from URL\", \"Load from Google Drive\"]\n",
        "#@markdown URL if downloading .npy file from website:\n",
        "URL_TO_PATCH_FILE = \"\" #@param {type:\"string\"}\n",
        "#@markdown Path if loading .npy file from Google Drive:\n",
        "DRIVE_PATH_TO_PATCH_FILE = \"\" #@param {type:\"string\"}\n",
        "\n",
        "def cached_url_download(url, format):\n",
        "  cache_filename = os.path.basename(url)\n",
        "  cache = pathlib.Path(cache_filename)\n",
        "  if not cache.is_file():\n",
        "    print(\"Downloading \" + cache_filename)\n",
        "    r = requests.get(url)\n",
        "    bytesio_object = io.BytesIO(r.content)\n",
        "    with open(cache_filename, \"wb\") as f:\n",
        "        f.write(bytesio_object.getbuffer())\n",
        "  else:\n",
        "    print(\"Using cached version of \" + cache_filename)\n",
        "  if format == \"numpy\":\n",
        "    return np.load(cache, allow_pickle=True)\n",
        "  elif format == \"image as RGB\":\n",
        "    return load_image(cache_filename, show=True)\n",
        "\n",
        "def upload_np_file():\n",
        "  uploaded = files.upload()\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n",
        "    with open(fn, 'rb') as f:\n",
        "      return np.load(f, allow_pickle = True)\n",
        "\n",
        "examples = {\"Fruit and veg\" : \"fruit.npy\", \n",
        "            \"Sea glass\" : \"shore_glass.npy\",\n",
        "            #\"Chrisantha\" : \"chrisantha2.npy\",\n",
        "            \"Handwritten MNIST\" : \"handwritten_mnist.npy\",\n",
        "            \"Animals\" : \"animals.npy\"}\n",
        "\n",
        "if PATCH_SET in examples:\n",
        "  repo_root = \"https://github.com/deepmind/arnheim/raw/main\"\n",
        "  segmented_data_initial = cached_url_download(\n",
        "      f\"{repo_root}/collage_patches/{examples[PATCH_SET]}\", format=\"numpy\")\n",
        "elif PATCH_SET == \"Load from URL\":\n",
        "  segmented_data_initial = cached_url_download(URL_TO_PATCH_FILE,\n",
        "                                               format=\"numpy\")\n",
        "elif PATCH_SET == \"Upload to Colab\":\n",
        "  segmented_data_initial = upload_np_file()\n",
        "else:  # \"Load from Google Drive\"\n",
        "  drive.mount(MOUNT_DIR)\n",
        "  data_file = pathlib.PurePath(MOUNT_DIR, \"MyDrive\", DRIVE_PATH_TO_PATCH_FILE)\n",
        "  print(\"Reading\", data_file)\n",
        "  segmented_data_initial = np.load(data_file, allow_pickle=True)\n",
        "\n",
        "\n",
        "segmented_data_initial_tmp = []\n",
        "for i in range(len(segmented_data_initial)):\n",
        "  if segmented_data_initial[i].shape[2] == 3:\n",
        "    segmented_data_initial_tmp.append(add_binary_alpha_mask(\n",
        "        segmented_data_initial[i]))\n",
        "  else:\n",
        "    segmented_data_initial_tmp.append(\n",
        "        segmented_data_initial[i])\n",
        "    \n",
        "segmented_data_initial = segmented_data_initial_tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg4ed9tyi6vZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Resize image patches to low- and high-res.\n",
        "\n",
        "#@markdown Scale patches to within (image size / PATCH_MAX_PROPORTION). \n",
        "#@markdown E.g. 5 produces small patches good for tiled images\n",
        "PATCH_MAX_PROPORTION =  5  #@param{type:\"slider\", min:2, max:8, step:1}\n",
        "\n",
        "#@markdown Alternatively, scale all patches by same amount\n",
        "FIXED_SCALE_PATCHES = False #@param {type:\"boolean\"}\n",
        "FIXED_SCALE_COEFF =   0.3#@param {type:\"number\"}\n",
        "\n",
        "#@markdown Brighten patches\n",
        "NORMALIZE_PATCH_BRIGHTNESS = False  #@param {type: \"boolean\"}\n",
        "#@markdown Show all the patches (useful for dubugging)\n",
        "SHOW_PATCHES = False #@param {type:\"boolean\"}\n",
        "\n",
        "PATCH_WIDTH_MIN = 16  \n",
        "PATCH_HEIGHT_MIN = 16 \n",
        "\n",
        "def normalise_patch_brightness(patch):\n",
        "  max_intensity = max(patch.max(), 1.0)\n",
        "  return ((patch / max_intensity) * 255).astype(np.uint8)\n",
        "\n",
        "# Permute the order of the segmented images. \n",
        "num_patches = len(segmented_data_initial)\n",
        "order = np.random.permutation(num_patches)\n",
        "\n",
        "# Compress all images until they are at most 1/PATCH_MAX_PROPORTION of the large\n",
        "# canvas size. \n",
        "hires_height = CANVAS_HEIGHT * MULTIPLIER_BIG_IMAGE\n",
        "hires_width = CANVAS_WIDTH * MULTIPLIER_BIG_IMAGE\n",
        "height_large_max = hires_height / PATCH_MAX_PROPORTION\n",
        "width_large_max = hires_width / PATCH_MAX_PROPORTION\n",
        "if FIXED_SCALE_PATCHES:\n",
        "  print(f\"Max size for fixed scale patches: ({hires_height},{hires_width})\")\n",
        "else:\n",
        "  print(\n",
        "      f\"Max patch size on large image: ({height_large_max}, {width_large_max})\")\n",
        "segmented_data = []\n",
        "segmented_data_high_res = []\n",
        "for patch_i in range(num_patches):\n",
        "  segmented_data_initial_i = segmented_data_initial[\n",
        "      order[patch_i]].astype(np.float32).swapaxes(0, 1)\n",
        "  shape_i = segmented_data_initial_i.shape\n",
        "  h_i = shape_i[0]\n",
        "  w_i = shape_i[1]\n",
        "  if h_i >= PATCH_HEIGHT_MIN and w_i >= PATCH_WIDTH_MIN:\n",
        "    # Coefficient for resizing the patch.\n",
        "    if FIXED_SCALE_PATCHES:\n",
        "      coeff_i_large = FIXED_SCALE_COEFF\n",
        "      if h_i * coeff_i_large > hires_height:\n",
        "        coeff_i_large = hires_height / h_i\n",
        "      if w_i * coeff_i_large > width_large_max:\n",
        "        coeff_i_large = min(coeff_i_large, hires_width / w_i)\n",
        "      if coeff_i_large != FIXED_SCALE_COEFF:\n",
        "        print(\n",
        "            f\"Patch {patch_i} too large; setting scaled to {coeff_i_large:.2f}\")\n",
        "    else:\n",
        "      coeff_i_large = 1.0\n",
        "      if h_i > height_large_max:\n",
        "        coeff_i_large = height_large_max / h_i\n",
        "      if w_i > width_large_max:\n",
        "        coeff_i_large = min(coeff_i_large, width_large_max / w_i)\n",
        "\n",
        "    # Resize the high-res patch?\n",
        "    if coeff_i_large < 1.0:\n",
        "      segmented_data_high_res_i = resize_patch(segmented_data_initial_i,\n",
        "                                              coeff_i_large)\n",
        "    else:\n",
        "      segmented_data_high_res_i = np.copy(segmented_data_initial_i)\n",
        " \n",
        "    # Resize the low-res patch.\n",
        "    coeff_i = coeff_i_large / MULTIPLIER_BIG_IMAGE\n",
        "    segmented_data_i = resize_patch(segmented_data_initial_i, coeff_i)\n",
        "    if (segmented_data_i.shape[0] > CANVAS_HEIGHT \n",
        "        or segmented_data_i.shape[1] > CANVAS_WIDTH):\n",
        "      print(f\"Patch size {segmented_data_i.shape} exceeds canvas size ({CANVAS_HEIGHT},{CANVAS_WIDTH})\")\n",
        "      import pdb; pdb.set_trace()\n",
        "    if NORMALIZE_PATCH_BRIGHTNESS:\n",
        "      segmented_data_i[...,:3] = normalise_patch_brightness(\n",
        "          segmented_data_i[...,:3])\n",
        "      segmented_data_high_res_i[...,:3] = normalise_patch_brightness(\n",
        "          segmented_data_high_res_i[...,:3])\n",
        "    segmented_data_high_res_i = segmented_data_high_res_i.astype(np.uint8)\n",
        "    segmented_data_high_res.append(segmented_data_high_res_i)\n",
        "    segmented_data_i = segmented_data_i.astype(np.uint8)\n",
        "    segmented_data.append(segmented_data_i)\n",
        "    print(\"{}/{}: initial {} -> small {}, large {} x{:.2f}\".format(\n",
        "        patch_i, num_patches, shape_i, segmented_data_i.shape, \n",
        "        segmented_data_high_res_i.shape,\n",
        "        coeff_i_large))\n",
        "  else:\n",
        "    print(f\"Discard patch of size {h_i}x{w_i}\")\n",
        "\n",
        "if SHOW_PATCHES:\n",
        "  print(\"Patch sizes during optimisation:\")\n",
        "  print_size_segmented_data(segmented_data)\n",
        "  print(\"Patch sizes for high-resolution final image:\")\n",
        "  print_size_segmented_data(segmented_data_high_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWV2a7ZekET"
      },
      "source": [
        "# Colour and affine transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxR0ZFpAebsH"
      },
      "source": [
        "## Affine transform classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "z1JiWgxJas5l"
      },
      "outputs": [],
      "source": [
        "class PopulationAffineTransforms(torch.nn.Module):\n",
        "  \"\"\"Population-based Affine Transform network.\"\"\"\n",
        "  def __init__(self, num_patches=1, pop_size=1):\n",
        "    super(PopulationAffineTransforms, self).__init__()\n",
        "\n",
        "    self._pop_size = pop_size\n",
        "    matrices_translation = (\n",
        "        np.random.rand(pop_size, num_patches, 2, 1) * (MAX_TRANS - MIN_TRANS) \n",
        "        + MIN_TRANS)\n",
        "    matrices_rotation = (\n",
        "        np.random.rand(pop_size, num_patches, 1, 1) * (MAX_ROT - MIN_ROT)\n",
        "        + MIN_ROT)\n",
        "    matrices_scale = (\n",
        "        np.random.rand(pop_size, num_patches, 1, 1) * (MAX_SCALE - MIN_SCALE) \n",
        "        + MIN_SCALE)\n",
        "    matrices_squeeze = (\n",
        "        np.random.rand(pop_size, num_patches, 1, 1) * (\n",
        "            (MAX_SQUEEZE - MIN_SQUEEZE) + MIN_SQUEEZE))\n",
        "    matrices_shear = (\n",
        "        np.random.rand(pop_size, num_patches, 1, 1) * (MAX_SHEAR - MIN_SHEAR) \n",
        "        + MIN_SHEAR)\n",
        "    self.translation = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_translation, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.rotation = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_rotation, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.scale = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_scale, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.squeeze = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_squeeze, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.shear = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_shear, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._identity = (\n",
        "        torch.ones((pop_size, num_patches, 1, 1)) * torch.eye(2).unsqueeze(0)\n",
        "        ).to(device)\n",
        "    self._zero_column = torch.zeros((pop_size, num_patches, 2, 1)).to(device)\n",
        "    self._unit_row = (\n",
        "        torch.ones((pop_size, num_patches, 1, 1)) * torch.tensor([0., 0., 1.])\n",
        "        ).to(device)\n",
        "    self._zeros = torch.zeros((pop_size, num_patches, 1, 1)).to(device)\n",
        "\n",
        "  def _clamp(self):\n",
        "    self.translation.data = self.translation.data.clamp(\n",
        "        min=MIN_TRANS, max=MAX_TRANS)\n",
        "    self.rotation.data = self.rotation.data.clamp(\n",
        "        min=MIN_ROT, max=MAX_ROT)\n",
        "    self.scale.data = self.scale.data.clamp(\n",
        "        min=MIN_SCALE, max=MAX_SCALE)\n",
        "    self.squeeze.data = self.squeeze.data.clamp(\n",
        "        min=MIN_SQUEEZE, max=MAX_SQUEEZE)\n",
        "    self.shear.data = self.shear.data.clamp(\n",
        "        min=MIN_SHEAR, max=MAX_SHEAR)\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    \"\"\"Copy parameters to child, mutating transform parameters.\"\"\"\n",
        "    with torch.no_grad():\n",
        "      self.translation[child, ...] = (self.translation[parent, ...] \n",
        "          + POS_AND_ROT_MUTATION_SCALE * torch.randn(\n",
        "              self.translation[child, ...].shape).to(device))\n",
        "      self.rotation[child, ...] = (self.rotation[parent, ...]  \n",
        "          + POS_AND_ROT_MUTATION_SCALE * torch.randn(\n",
        "              self.rotation[child, ...].shape).to(device))\n",
        "      self.scale[child, ...] = (self.scale[parent, ...] \n",
        "          + SCALE_MUTATION_SCALE * torch.randn(\n",
        "              self.scale[child, ...].shape).to(device))\n",
        "      self.squeeze[child, ...] = (self.squeeze[parent, ...]\n",
        "          + DISTORT_MUTATION_SCALE * torch.randn(\n",
        "              self.squeeze[child, ...].shape).to(device))\n",
        "      self.shear[child, ...] = (self.shear[parent, ...]\n",
        "          + DISTORT_MUTATION_SCALE * torch.randn(\n",
        "              self.shear[child, ...].shape).to(device))\n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other spatial transform, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.translation[idx_to, ...] = other.translation[idx_from, ...]\n",
        "      self.rotation[idx_to, ...] = other.rotation[idx_from, ...]\n",
        "      self.scale[idx_to, ...] = other.scale[idx_from, ...]\n",
        "      self.squeeze[idx_to, ...] = other.squeeze[idx_from, ...]\n",
        "      self.shear[idx_to, ...] = other.shear[idx_from, ...]\n",
        "\n",
        "  def forward(self, x):\n",
        "    self._clamp()\n",
        "    scale_affine_mat = torch.cat([\n",
        "        torch.cat([self.scale, self.shear], 3),\n",
        "        torch.cat([self._zeros, self.scale * self.squeeze], 3)],\n",
        "        2)\n",
        "    scale_affine_mat = torch.cat([\n",
        "        torch.cat([scale_affine_mat, self._zero_column], 3),\n",
        "        self._unit_row], 2)\n",
        "    rotation_affine_mat = torch.cat([\n",
        "        torch.cat([torch.cos(self.rotation), -torch.sin(self.rotation)], 3),\n",
        "        torch.cat([torch.sin(self.rotation), torch.cos(self.rotation)], 3)],\n",
        "        2)\n",
        "    rotation_affine_mat = torch.cat([\n",
        "        torch.cat([rotation_affine_mat, self._zero_column], 3),\n",
        "        self._unit_row], 2)\n",
        "    \n",
        "    scale_rotation_mat = torch.matmul(scale_affine_mat,\n",
        "                                      rotation_affine_mat)[:, :, :2, :]\n",
        "    # Population and patch dimensions (0 and 1) need to be merged.\n",
        "    # E.g. from (POP_SIZE, NUM_PATCHES, CHANNELS, WIDTH, HEIGHT) \n",
        "    # to (POP_SIZE * NUM_PATCHES, CHANNELS, WIDTH, HEIGHT)\n",
        "    scale_rotation_mat = scale_rotation_mat[:, :, :2, :].view(\n",
        "        1, -1, *(scale_rotation_mat[:, :, :2, :].size()[2:])).squeeze()\n",
        "    x = x.view(1, -1, *(x.size()[2:])).squeeze()\n",
        "    scaled_rotated_grid = F.affine_grid(\n",
        "        scale_rotation_mat, x.size(), align_corners=True)\n",
        "    scaled_rotated_x = F.grid_sample(x, scaled_rotated_grid, align_corners=True)\n",
        "\n",
        "    translation_affine_mat = torch.cat([self._identity, self.translation], 3)\n",
        "    translation_affine_mat = translation_affine_mat.view(\n",
        "        1, -1, *(translation_affine_mat.size()[2:])).squeeze()\n",
        "    translated_grid = F.affine_grid(\n",
        "        translation_affine_mat, x.size(), align_corners=True)\n",
        "    y = F.grid_sample(scaled_rotated_x, translated_grid, align_corners=True)\n",
        "    return y.view(self._pop_size, NUM_PATCHES, *(y.size()[1:]))\n",
        "\n",
        "  def tensor_to(self, device):\n",
        "    self.translation = self.translation.to(device)\n",
        "    self.rotation = self.rotation.to(device)\n",
        "    self.scale = self.scale.to(device)\n",
        "    self.squeeze = self.squeeze.to(device)\n",
        "    self.shear = self.shear.to(device)\n",
        "    self._identity = self._identity.to(device)\n",
        "    self._zero_column = self._zero_column.to(device)\n",
        "    self._unit_row = self._unit_row.to(device)\n",
        "    self._zeros = self._zeros.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9GsjxMcgAP7"
      },
      "source": [
        "## RGB and HSV color transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRc63pIMNwtc"
      },
      "outputs": [],
      "source": [
        "class PopulationOrderOnlyTransforms(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, num_patches=1, pop_size=1):\n",
        "    super(PopulationOrderOnlyTransforms, self).__init__()\n",
        "\n",
        "    self._pop_size = pop_size\n",
        "\n",
        "    population_zeros = np.ones((pop_size, num_patches, 1, 1, 1))\n",
        "    population_orders = np.random.rand(pop_size, num_patches, 1, 1, 1)\n",
        "\n",
        "    self._zeros = torch.nn.Parameter(\n",
        "        torch.tensor(population_zeros, dtype=torch.float),\n",
        "        requires_grad=False)\n",
        "    self.orders = torch.nn.Parameter(\n",
        "        torch.tensor(population_orders, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._hsv_to_rgb = hsv.HsvToRgb()\n",
        "\n",
        "  def _clamp(self):\n",
        "    self.orders.data = self.orders.data.clamp(min=0.0, max=1.0)\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    with torch.no_grad():\n",
        "      self.orders[child, ...] = self.orders[parent, ...]\n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other colour transform, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.orders[idx_to, ...] = other.orders[idx_from, ...]\n",
        "\n",
        "  def forward(self, x):\n",
        "    self._clamp()\n",
        "    colours = torch.cat(\n",
        "        [self._zeros, self._zeros, self._zeros, self._zeros, self.orders],\n",
        "        2)\n",
        "    return colours * x\n",
        "\n",
        "  def tensor_to(self, device):\n",
        "    self.orders = self.orders.to(device)\n",
        "    self._zeros = self._zeros.to(device)\n",
        "\n",
        "\n",
        "class PopulationColourHSVTransforms(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, num_patches=1, pop_size=1):\n",
        "    super(PopulationColourHSVTransforms, self).__init__()\n",
        "\n",
        "    print('PopulationColourHSVTransforms for {} patches, {} individuals'.format(\n",
        "        num_patches, pop_size))\n",
        "    self._pop_size = pop_size\n",
        "\n",
        "    coeff_hue = 0.5 * (MAX_HUE - MIN_HUE) + MIN_HUE\n",
        "    coeff_sat = 0.5 * (MAX_SAT - MIN_SAT) + MIN_SAT\n",
        "    coeff_val = 0.5 * (MAX_VAL - MIN_VAL) + MIN_VAL\n",
        "    population_hues = np.random.rand(pop_size, num_patches, 1, 1, 1) * coeff_hue\n",
        "    population_saturations = np.random.rand(\n",
        "        pop_size, num_patches, 1, 1, 1) * coeff_sat\n",
        "    population_values = np.random.rand(\n",
        "        pop_size, num_patches, 1, 1, 1) * coeff_val\n",
        "    population_zeros = np.ones((pop_size, num_patches, 1, 1, 1))\n",
        "    population_orders = np.random.rand(pop_size, num_patches, 1, 1, 1)\n",
        "    \n",
        "    self.hues = torch.nn.Parameter(\n",
        "        torch.tensor(population_hues, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.saturations = torch.nn.Parameter(\n",
        "        torch.tensor(population_saturations, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.values = torch.nn.Parameter(\n",
        "        torch.tensor(population_values, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._zeros = torch.nn.Parameter(\n",
        "        torch.tensor(population_zeros, dtype=torch.float),\n",
        "        requires_grad=False)\n",
        "    self.orders = torch.nn.Parameter(\n",
        "        torch.tensor(population_orders, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._hsv_to_rgb = hsv.HsvToRgb()\n",
        "\n",
        "  def _clamp(self):\n",
        "    self.hues.data = self.hues.data.clamp(min=MIN_HUE, max=MAX_HUE)\n",
        "    self.saturations.data = self.saturations.data.clamp(\n",
        "        min=MIN_SAT, max=MAX_SAT)\n",
        "    self.values.data = self.values.data.clamp(min=MIN_VAL, max=MAX_VAL)\n",
        "    self.orders.data = self.orders.data.clamp(min=0.0, max=1.0)\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    with torch.no_grad():\n",
        "      self.hues[child, ...] = (\n",
        "          self.hues[parent, ...]\n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.hues[child, ...].shape).to(device))\n",
        "      self.saturations[child, ...] = (\n",
        "          self.saturations[parent, ...]\n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.saturations[child, ...].shape).to(device))\n",
        "      self.values[child, ...] = (\n",
        "          self.values[parent, ...]\n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.values[child, ...].shape).to(device))\n",
        "      self.orders[child, ...] = self.orders[parent, ...]\n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other colour transform, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.hues[idx_to, ...] = other.hues[idx_from, ...]\n",
        "      self.saturations[idx_to, ...] = other.saturations[idx_from, ...]\n",
        "      self.values[idx_to, ...] = other.values[idx_from, ...]\n",
        "      self.orders[idx_to, ...] = other.orders[idx_from, ...]\n",
        "\n",
        "  def forward(self, image):\n",
        "    self._clamp()\n",
        "    colours = torch.cat(\n",
        "        [self.hues, self.saturations, self.values, self._zeros, self.orders], 2)\n",
        "    hsv_image = colours * image\n",
        "    rgb_image = self._hsv_to_rgb(hsv_image[:, :, :3, :, :])\n",
        "    return torch.cat([rgb_image, hsv_image[:, :, 3:, :, :]], axis=2)\n",
        "\n",
        "  def tensor_to(self, device):\n",
        "    self.hues = self.hues.to(device)\n",
        "    self.saturations = self.saturations.to(device)\n",
        "    self.values = self.values.to(device)\n",
        "    self.orders = self.orders.to(device)\n",
        "    self._zeros = self._zeros.to(device)\n",
        "\n",
        "\n",
        "class PopulationColourRGBTransforms(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, num_patches=1, pop_size=1):\n",
        "    super(PopulationColourRGBTransforms, self).__init__()\n",
        "\n",
        "    print('PopulationColourRGBTransforms for {} patches, {} individuals'.format(\n",
        "        num_patches, pop_size))\n",
        "    self._pop_size = pop_size\n",
        "\n",
        "    rgb_init_range = INITIAL_MAX_RGB - INITIAL_MIN_RGB\n",
        "    population_reds = (np.random.rand(pop_size, num_patches, 1, 1, 1) \n",
        "        * rgb_init_range) + INITIAL_MIN_RGB\n",
        "    population_greens = (np.random.rand(\n",
        "        pop_size, num_patches, 1, 1, 1) * rgb_init_range) + INITIAL_MIN_RGB\n",
        "    population_blues = (np.random.rand(\n",
        "        pop_size, num_patches, 1, 1, 1) * rgb_init_range) + INITIAL_MIN_RGB\n",
        "    population_zeros = np.ones((pop_size, num_patches, 1, 1, 1))\n",
        "    population_orders = np.random.rand(pop_size, num_patches, 1, 1, 1)\n",
        "\n",
        "    self.reds = torch.nn.Parameter(\n",
        "        torch.tensor(population_reds, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.greens = torch.nn.Parameter(\n",
        "        torch.tensor(population_greens, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.blues = torch.nn.Parameter(\n",
        "        torch.tensor(population_blues, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._zeros = torch.nn.Parameter(\n",
        "        torch.tensor(population_zeros, dtype=torch.float),\n",
        "        requires_grad=False)\n",
        "    self.orders = torch.nn.Parameter(\n",
        "        torch.tensor(population_orders, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "\n",
        "  def _clamp(self):\n",
        "    self.reds.data = self.reds.data.clamp(min=MIN_RGB, max=MAX_RGB)\n",
        "    self.greens.data = self.greens.data.clamp(min=MIN_RGB, max=MAX_RGB)\n",
        "    self.blues.data = self.blues.data.clamp(min=MIN_RGB, max=MAX_RGB)\n",
        "    self.orders.data = self.orders.data.clamp(min=0.0, max=1.0)\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    with torch.no_grad():\n",
        "      self.reds[child, ...] = (\n",
        "          self.reds[parent, ...] \n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.reds[child, ...].shape).to(device))\n",
        "      self.greens[child, ...] = (\n",
        "          self.greens[parent, ...] \n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.greens[child, ...].shape).to(device))\n",
        "      self.blues[child, ...] = (\n",
        "          self.blues[parent, ...] \n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.blues[child, ...].shape).to(device))\n",
        "      self.orders[child, ...] = self.orders[parent, ...] \n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other colour transform, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.reds[idx_to, ...] = other.reds[idx_from, ...]\n",
        "      self.greens[idx_to, ...] = other.greens[idx_from, ...]\n",
        "      self.blues[idx_to, ...] = other.blues[idx_from, ...]\n",
        "      self.orders[idx_to, ...] = other.orders[idx_from, ...]\n",
        "\n",
        "  def forward(self, x):\n",
        "    self._clamp()\n",
        "    colours = torch.cat(\n",
        "        [self.reds, self.greens, self.blues, self._zeros, self.orders], 2)\n",
        "    return colours * x\n",
        "\n",
        "  def tensor_to(self, device):\n",
        "    self.reds = self.reds.to(device)\n",
        "    self.greens = self.greens.to(device)\n",
        "    self.blues = self.blues.to(device)\n",
        "    self.orders = self.orders.to(device)\n",
        "    self._zeros = self._zeros.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubwSr59fgk0C"
      },
      "source": [
        "# Rendering functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "GCro-8edu6WX"
      },
      "outputs": [],
      "source": [
        "RENDER_EPSILON = 1e-8\n",
        "RENDER_OVERLAP_TEMPERATURE = 0.1\n",
        "RENDER_OVERLAP_ZERO_OFFSET = -5\n",
        "RENDER_OVERLAP_MASK_THRESHOLD = 0.5\n",
        "RENDER_TRANSPARENCY_MASK_THRESHOLD = 0.1\n",
        "\n",
        "\n",
        "def population_render_transparency(x, b=None):\n",
        "  \"\"\"Image rendering function that renders all patches on top of one another,\n",
        "     with transparency, using black as the transparent colour.\n",
        "\n",
        "  Args:\n",
        "    x: tensor of transformed RGB image patches of shape [S, B, 5, H, W].\n",
        "    b: optional tensor of background RGB image of shape [S, 3, H, W].\n",
        "  Returns:\n",
        "    Tensor of rendered RGB images of shape [S, 3, H, W].\n",
        "  \"\"\"\n",
        "  # Sum the RGB patches [S, B, 3, H, W] as [S, 3, H, W].\n",
        "  x = x[:, :, :3, :, :] * x[:, :, 3:4, :, :]\n",
        "  y = x[:, :, :3, :, :].sum(1)\n",
        "  if INVERT_COLOURS:\n",
        "    y[:, :3, :, :] = 1.0 - y[:, :3, :, :]\n",
        "  # Add backgrounds [S, 3, H, W].\n",
        "  if b is not None:\n",
        "    b = b.cuda() if x.is_cuda else b.cpu()\n",
        "    y = torch.where(y.sum(1, keepdim=True) > RENDER_TRANSPARENCY_MASK_THRESHOLD,\n",
        "                    y[:, :3, :, :], b.unsqueeze(0)[:, :3, :, :])\n",
        "  return y.clamp(0., 1.).permute(0, 2, 3, 1)\n",
        "\n",
        "\n",
        "def population_render_masked_transparency(x, mode, b=None):\n",
        "  \"\"\"Image rendering function that renders all patches on top of one another,\n",
        "     with transparency, using the alpha chanel as the mask colour.\n",
        "\n",
        "  Args:\n",
        "    x: tensor of transformed RGB image patches of shape [S, B, 5, H, W].\n",
        "    mode: [\"clipped\"|\"normed\"] method of handling alpha with background.\n",
        "    b: optional tensor of background RGB image of shape [S, 3, H, W].\n",
        "  Returns:\n",
        "    Tensor of rendered RGB images of shape [S, 3, H, W].\n",
        "  \"\"\"\n",
        "  # Get the patch mask [S, B, 1, H, W] and sum of masks [S, 1, H, W].\n",
        "  mask = x[:, :, 3:4, :, :]\n",
        "  mask_sum = mask.sum(1) + RENDER_EPSILON\n",
        "  # Mask the RGB patches [S, B, 4, H, W] -> [S, B, 3, H, W].\n",
        "  masked_x = x[:, :, :3, :, :] * mask\n",
        "  # Compute mean of the RGB patches [S, B, 3, H, W] as [S, 3, H, W].\n",
        "  x_sum = masked_x.sum(1)\n",
        "  y = torch.where(\n",
        "      mask_sum > RENDER_EPSILON, x_sum / mask_sum, mask_sum)\n",
        "  # Anti-aliasing on the countours of the sum of patches.\n",
        "  # y = y * mask_sum.clamp(0., 1.)\n",
        "  if INVERT_COLOURS:\n",
        "    y[:, :3, :, :] = 1.0 - y[:, :3, :, :]\n",
        "  # Add backgrounds [S, 3, H, W].\n",
        "  if b is not None:\n",
        "    b = b.cuda() if x.is_cuda else b.cpu()\n",
        "    if mode == \"normed\":\n",
        "      mask_max = mask_sum.max(\n",
        "          dim=2, keepdim=True).values.max(dim=3, keepdim=True).values\n",
        "      mask = mask_sum / mask_max\n",
        "    elif mode == \"clipped\":\n",
        "      mask = mask_sum.clamp(0., 1.)\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown masked_transparency mode {mode}\")\n",
        "    y = y[:, :3, :, :] * mask + b.unsqueeze(0)[:, :3, :, :] * (1 - mask)\n",
        "  return y.clamp(0., 1.).permute(0, 2, 3, 1)\n",
        "\n",
        "\n",
        "def population_render_overlap(x, b=None, gamma=None):\n",
        "  \"\"\"Image rendering function that overlays all patches on top of one another,\n",
        "     with semi-translucent overlap, using the alpha chanel as the mask colour\n",
        "     and the 5th channel as the order for the overlapped images.\n",
        "\n",
        "  Args:\n",
        "    x: tensor of transformed RGB image patches of shape [S, B, 5, H, W].\n",
        "    b: optional tensor of background RGB image of shape [S, 3, H, W].\n",
        "  Returns:\n",
        "    Tensor of rendered RGB images of shape [S, 3, H, W].\n",
        "  \"\"\"\n",
        "  # Get the patch mask [S, B, 1, H, W].\n",
        "  mask = x[:, :, 3:4, :, :]\n",
        "  # Mask the patches [S, B, 4, H, W] -> [S, B, 3, H, W]\n",
        "  masked_x = x[:, :, :3, :, :] * mask * mask\n",
        "  # Mask the orders [S, B, 1, H, W] -> [S, B, 1, H, W]\n",
        "  order = torch.where(\n",
        "      mask > RENDER_OVERLAP_MASK_THRESHOLD,\n",
        "      x[:, :, 4:, :, :] * mask / RENDER_OVERLAP_TEMPERATURE,\n",
        "      mask + RENDER_OVERLAP_ZERO_OFFSET)\n",
        "  # Get weights from orders [S, B, 1, H, W]\n",
        "  weights = F.softmax(order, dim=1)\n",
        "  # Apply weights to masked patches and compute mean over patches [S, 3, H, W].\n",
        "  y = (weights * masked_x).sum(1)\n",
        "  if INVERT_COLOURS:\n",
        "    y[:, :3, :, :] = 1.0 - y[:, :3, :, :]\n",
        "  if b is not None:\n",
        "    b = b.cuda() if x.is_cuda else b.cpu()\n",
        "    y = torch.where(mask.sum(1) > RENDER_OVERLAP_MASK_THRESHOLD, y[:, :3, :, :],\n",
        "                  b.unsqueeze(0)[:, :3, :, :])\n",
        "  return y.clamp(0., 1.).permute(0, 2, 3, 1)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ckYmVuAAO7x"
      },
      "source": [
        "# Collage network definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "zrReSIFc6tOq"
      },
      "outputs": [],
      "source": [
        "class PopulationCollage(torch.nn.Module):\n",
        "  \"\"\"Population-based segmentation collage network.\n",
        "  \n",
        "  Image structure in this class is SCHW.\"\"\"\n",
        "  def __init__(self,\n",
        "               pop_size=1,\n",
        "               is_high_res=False,\n",
        "               segmented_data=None,\n",
        "               background_image=None):\n",
        "    \"\"\"Constructor, relying on global parameters.\"\"\"\n",
        "    super(PopulationCollage, self).__init__()\n",
        "\n",
        "    # Population size.\n",
        "    self._pop_size = pop_size\n",
        "\n",
        "    # Create the spatial transformer and colour transformer for patches.\n",
        "    self.spatial_transformer = PopulationAffineTransforms(\n",
        "        num_patches=NUM_PATCHES, pop_size=pop_size).cuda()\n",
        "    if COLOUR_TRANSFORMATIONS == \"HSV space\":\n",
        "      self.colour_transformer = PopulationColourHSVTransforms(\n",
        "          num_patches=NUM_PATCHES, pop_size=pop_size).cuda()\n",
        "    elif COLOUR_TRANSFORMATIONS == \"RGB space\":\n",
        "      self.colour_transformer = PopulationColourRGBTransforms(\n",
        "          num_patches=NUM_PATCHES, pop_size=pop_size).cuda()\n",
        "    else:\n",
        "      self.colour_transformer = PopulationOrderOnlyTransforms(\n",
        "          num_patches=NUM_PATCHES, pop_size=pop_size).cuda()\n",
        "\n",
        "    # Optimisation is run in low-res, final rendering is in high-res.\n",
        "    self._high_res = is_high_res\n",
        "\n",
        "    # Store the background image (low- and high-res).\n",
        "    self._background_image = background_image\n",
        "    if self._background_image is not None:\n",
        "      print(f'Background image of size {self._background_image.shape}')\n",
        "\n",
        "    # Store the dataset (low- and high-res).\n",
        "    self._dataset = segmented_data\n",
        "    #print(f'There are {len(self._dataset)} image patches in the dataset')\n",
        "\n",
        "    # Initial set of indices, pointing to the NUM_PATCHES first dataset images. \n",
        "    self.patch_indices = [np.arange(NUM_PATCHES) % len(self._dataset)\n",
        "                          for _ in range(pop_size)]\n",
        "\n",
        "    # Patches in low and high-res.\n",
        "    self.patches = None\n",
        "    self.store_patches()\n",
        "\n",
        "  def store_patches(self, population_idx=None):\n",
        "    \"\"\"Store the image patches for each population element.\"\"\"\n",
        "    t0 = time.time()\n",
        "\n",
        "    if population_idx is not None and self.patches is not None:\n",
        "      list_indices = [population_idx]\n",
        "      #print(f'Reload {NUM_PATCHES} image patches for [{population_idx}]')\n",
        "      self.patches[population_idx, :, :4, :, :] = 0\n",
        "    else:\n",
        "      list_indices = np.arange(self._pop_size)\n",
        "      #print(f'Store {NUM_PATCHES} image patches for [1, ..., {self._pop_size}]')\n",
        "      if self._high_res:\n",
        "        self.patches = torch.zeros(\n",
        "            1, NUM_PATCHES, 5, CANVAS_HEIGHT * MULTIPLIER_BIG_IMAGE,\n",
        "            CANVAS_WIDTH * MULTIPLIER_BIG_IMAGE).to('cpu')\n",
        "      else:\n",
        "        self.patches = torch.zeros(\n",
        "            self._pop_size, NUM_PATCHES, 5, CANVAS_HEIGHT, CANVAS_WIDTH\n",
        "            ).to(device)\n",
        "      self.patches[:, :, 4, :, :] = 1.0\n",
        "\n",
        "    # Put the segmented data into the patches.\n",
        "    for i in list_indices:\n",
        "      for j in range(NUM_PATCHES):\n",
        "        k = self.patch_indices[i][j]\n",
        "        patch_j = torch.tensor(\n",
        "            self._dataset[k].swapaxes(0, 2) / 255.0).to(device)\n",
        "        width_j = patch_j.shape[1]\n",
        "        height_j = patch_j.shape[2]\n",
        "        if self._high_res:\n",
        "          w0 = int((CANVAS_WIDTH * MULTIPLIER_BIG_IMAGE - width_j) / 2.0)\n",
        "          h0 = int((CANVAS_HEIGHT * MULTIPLIER_BIG_IMAGE - height_j) / 2.0)\n",
        "        else:\n",
        "          w0 = int((CANVAS_WIDTH - width_j) / 2.0)\n",
        "          h0 = int((CANVAS_HEIGHT - height_j) / 2.0)\n",
        "        if w0 < 0 or h0 < 0:\n",
        "          import pdb; pdb.set_trace()\n",
        "        self.patches[i, j, :4, w0:(w0 + width_j), h0:(h0 + height_j)] = patch_j\n",
        "    t1 = time.time()\n",
        "    #print('Updated patches in {:.3f}s'.format(t1-t0))\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    with torch.no_grad():\n",
        "      # Copy the patches indices from the parent to the child.\n",
        "      self.patch_indices[child] = copy.deepcopy(self.patch_indices[parent])\n",
        "      \n",
        "      # Mutate the child patches with a single swap from the original dataset. \n",
        "      if PATCH_MUTATION_PROBABILITY > np.random.uniform():\n",
        "        idx_dataset  = np.random.randint(len(self._dataset))\n",
        "        idx_patch  = np.random.randint(NUM_PATCHES)\n",
        "        self.patch_indices[child][idx_patch] = idx_dataset\n",
        "\n",
        "      # Update all the patches for the child.\n",
        "      self.store_patches(child)\n",
        "  \n",
        "      self.spatial_transformer.copy_and_mutate_s(parent, child)\n",
        "      self.colour_transformer.copy_and_mutate_s(parent, child)\n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other collage generator, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.patch_indices[idx_to] = copy.deepcopy(other.patch_indices[idx_from])\n",
        "      self.store_patches(idx_to)\n",
        "      self.spatial_transformer.copy_from(\n",
        "          other.spatial_transformer, idx_to, idx_from)\n",
        "      self.colour_transformer.copy_from(\n",
        "          other.colour_transformer, idx_to, idx_from)\n",
        "\n",
        "  def forward(self, params=None):\n",
        "    \"\"\"Input-less forward function.\"\"\"\n",
        "\n",
        "    shifted_patches = self.spatial_transformer(self.patches)\n",
        "    background_image = self._background_image\n",
        "    coloured_patches = self.colour_transformer(shifted_patches)\n",
        "    if RENDER_METHOD == \"transparency\":\n",
        "      img = population_render_transparency(coloured_patches, background_image)\n",
        "    elif RENDER_METHOD == \"masked_transparency_clipped\":\n",
        "      img = population_render_masked_transparency(\n",
        "          coloured_patches, \"clipped\", background_image)\n",
        "    elif RENDER_METHOD == \"masked_transparency_normed\":\n",
        "      img = population_render_masked_transparency(\n",
        "          coloured_patches, \"normed\", background_image)\n",
        "    elif RENDER_METHOD == \"opacity\":\n",
        "      if params is not None and 'gamma' in params:\n",
        "        gamma = params['gamma']\n",
        "      else:\n",
        "        gamma = None\n",
        "      img = population_render_overlap(\n",
        "          coloured_patches, background_image)\n",
        "    else:\n",
        "      print(\"Unhandled render method\")\n",
        "    return img\n",
        "\n",
        "  def tensors_to(self, device):\n",
        "    self.spatial_transformer.tensor_to(device)\n",
        "    self.colour_transformer.tensor_to(device)\n",
        "    self.patches = self.patches.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIXzueO3PB-4"
      },
      "source": [
        "# Image and video function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "urXU9M5hS6H0"
      },
      "outputs": [],
      "source": [
        "#@title Image rendering and display\n",
        "\n",
        "def layout_img_batch(img_batch, max_display=None):\n",
        "    # img_batch.shape = (7, 224, 224, 3)  S, H, W, C\n",
        "    img_np = img_batch.transpose(0, 2, 1, 3).clip(0.0, 1.0)  # S, W, H, C\n",
        "    if max_display:\n",
        "      img_np = img_np[:max_display, ...]\n",
        "    sp = img_np.shape\n",
        "    img_np[:, 0, :, :] = 1.0  # White line separator \n",
        "    img_stitch = np.reshape(img_np, (sp[1] * sp[0], sp[2], sp[3]))\n",
        "    img_r = img_stitch.transpose(1, 0, 2)   # H, W, C\n",
        "    return img_r\n",
        "\n",
        "def show_and_save(img_batch, t=None,\n",
        "                  max_display=1, interpolation=\"None\", stitch=True,\n",
        "                  img_format=\"SCHW\", show=True, filename=None):\n",
        "  \"\"\"Display image.\n",
        "\n",
        "  Args:\n",
        "  \n",
        "    img: image to display\n",
        "    t: time step\n",
        "    max_display: max number of images to display from population\n",
        "    interpolation: interpolate enlarged images\n",
        "    stitch: append images side-by-side\n",
        "    img_format: SHWC or SCHW (the latter used by CLIP)\n",
        "\n",
        "  Returns:\n",
        "    stitched image or None\n",
        "  \"\"\"\n",
        "\n",
        "  if isinstance(img_batch, torch.Tensor):\n",
        "    img_np = img_batch.detach().cpu().numpy()\n",
        "  else:\n",
        "    img_np = img_batch\n",
        "\n",
        "  if len(img_np.shape) == 3:\n",
        "    # if not a batch make it one.\n",
        "    img_np = np.expand_dims(img_np, axis=0)\n",
        "\n",
        "  if not stitch:\n",
        "    # print(f\"image (not stitch) min {img_np.min()}, max {img_np.max()}\")\n",
        "    for i in range(min(max_display, img_np.shape[0])):\n",
        "      img = img_np[i]\n",
        "      if img_format == \"SCHW\":  # Convert to SHWC\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "      img = np.clip(img, 0.0, 1.0)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) * 255\n",
        "      if filename:\n",
        "        if img.shape[1] > CANVAS_WIDTH:\n",
        "          filename = \"_highres_\" + filename \n",
        "        filename = f\"{DIR_RESULTS}/{filename}_{str(i)}\"\n",
        "        if t is not None:\n",
        "          filename += \"_t_\" + str(t)\n",
        "        filename += \".png\"\n",
        "        print(f\"Saving image {filename} (shape={img.shape})\")\n",
        "        cv2.imwrite(filename, img)\n",
        "      if show:\n",
        "        cv2_imshow(img)\n",
        "    return None\n",
        "  else:\n",
        "    # print(f\"image (stitch) min {img_np.min()}, max {img_np.max()}\")\n",
        "    img_np = np.clip(img_np, 0.0, 1.0)\n",
        "    num_images = img_np.shape[0]\n",
        "    if img_format == \"SCHW\":  # Convert to SHWC\n",
        "      img_np = img_np.transpose((0, 2, 3, 1))\n",
        "    laid_out = layout_img_batch(img_np, max_display)\n",
        "    if show:\n",
        "      cv2_imshow(cv2.cvtColor(laid_out, cv2.COLOR_BGR2RGB) * 255)\n",
        "    return laid_out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvMiX61xGEOS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Video creator {vertical-output: true}\n",
        "\n",
        "class VideoWriter:\n",
        "  \"\"\"Create a video from image frames.\"\"\"\n",
        "\n",
        "  def __init__(self, filename=\"_autoplay.mp4\", fps=20.0, **kw):\n",
        "    \"\"\"Video creator.\n",
        "\n",
        "    Creates and display a video made from frames. The default\n",
        "    filename causes the video to be displayed on exit.\n",
        "\n",
        "    Args:\n",
        "      filename: name of video file\n",
        "      fps: frames per second for video\n",
        "      **kw: args to be passed to FFMPEG_VideoWriter\n",
        "\n",
        "    Returns:\n",
        "      VideoWriter instance.\n",
        "    \"\"\"\n",
        "\n",
        "    self.writer = None\n",
        "    self.params = dict(filename=filename, fps=fps, **kw)\n",
        "\n",
        "  def add(self, img):\n",
        "    \"\"\"Add image to video.\n",
        "\n",
        "    Add new frame to image file, creating VideoWriter if requried.\n",
        "\n",
        "    Args:\n",
        "      img: array-like frame, shape [X, Y, 3] or [X, Y]\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "\n",
        "    img = np.asarray(img)\n",
        "    if self.writer is None:\n",
        "      h, w = img.shape[:2]\n",
        "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
        "    if img.dtype in [np.float32, np.float64]:\n",
        "      img = np.uint8(img.clip(0, 1)*255)\n",
        "    if len(img.shape) == 2:\n",
        "      img = np.repeat(img[..., None], 3, -1)\n",
        "    self.writer.write_frame(img)\n",
        "\n",
        "  def close(self):\n",
        "    if self.writer:\n",
        "      self.writer.close()\n",
        "\n",
        "  def __enter__(self):\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, *kw):\n",
        "    self.close()\n",
        "    if self.params[\"filename\"] == \"_autoplay.mp4\":\n",
        "      self.show()\n",
        "\n",
        "  def show(self, **kw):\n",
        "    \"\"\"Display video.\n",
        "\n",
        "    Args:\n",
        "      **kw: args to be passed to mvp.ipython_display\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    self.close()\n",
        "    fn = self.params[\"filename\"]\n",
        "    display(mvp.ipython_display(fn, **kw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pUbeONI1d8z",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Metadata export\n",
        "\n",
        "def export_metadata(metadata_filename):\n",
        "  metadata = {}\n",
        "  metadata_canvas = {\"CANVAS_WIDTH\": CANVAS_WIDTH,\n",
        "                    \"CANVAS_HEIGHT\": CANVAS_HEIGHT,\n",
        "                    \"MULTIPLIER_BIG_IMAGE\": MULTIPLIER_BIG_IMAGE,\n",
        "                    \"PATCH_WIDTH_MIN\": PATCH_WIDTH_MIN,\n",
        "                    \"PATCH_HEIGHT_MIN\": PATCH_HEIGHT_MIN,\n",
        "                    \"PATCH_MAX_PROPORTION\": PATCH_MAX_PROPORTION}\n",
        "  metadata = {\"canvas\": metadata_canvas}\n",
        "  metadata_collage = {\"RENDER_METHOD\": RENDER_METHOD,\n",
        "                      \"NUM_PATCHES\": NUM_PATCHES,\n",
        "                      \"COLOUR_TRANSFORMATIONS\": COLOUR_TRANSFORMATIONS,\n",
        "                      \"INVERT_COLOURS\": INVERT_COLOURS}\n",
        "  metadata[\"collage\"] = metadata_collage\n",
        "  metadata_affine = {\"MIN_TRANS\": MIN_TRANS,\n",
        "                    \"MAX_TRANS\": MAX_TRANS,\n",
        "                    \"MIN_SCALE\": MIN_SCALE,\n",
        "                    \"MAX_SCALE\": MAX_SCALE,\n",
        "                    \"MIN_SQUEEZE\": MIN_SQUEEZE,\n",
        "                    \"MAX_SQUEEZE\": MAX_SQUEEZE,\n",
        "                    \"MIN_SHEAR\": MIN_SHEAR,\n",
        "                    \"MAX_SHEAR\": MAX_SHEAR,\n",
        "                    \"MIN_ROT_DEG\": MIN_ROT_DEG,\n",
        "                    \"MAX_ROT_DEG\": MAX_ROT_DEG,\n",
        "                    \"MIN_ROT\": MIN_ROT,\n",
        "                    \"MAX_ROT\": MAX_ROT}\n",
        "  metadata[\"affine\"] = metadata_affine\n",
        "  metadata_colour = {\"MIN_RGB\": MIN_RGB,\n",
        "                    \"MAX_RGB\": MAX_RGB,\n",
        "                    \"MIN_HUE\": MIN_HUE,\n",
        "                    \"MAX_HUE_DEG\": MAX_HUE_DEG,\n",
        "                    \"MAX_HUE\": MAX_HUE,\n",
        "                    \"MIN_SAT\": MIN_SAT,\n",
        "                    \"MAX_SAT\": MAX_SAT,\n",
        "                    \"MIN_VAL\": MIN_VAL,\n",
        "                    \"MAX_VAL\": MAX_VAL}\n",
        "  metadata[\"colour\"] = metadata_colour\n",
        "  metadata_training = {\"OPTIM_STEPS\": OPTIM_STEPS,\n",
        "                      \"LEARNING_RATE\": LEARNING_RATE,\n",
        "                      \"USE_IMAGE_AUGMENTATIONS\": USE_IMAGE_AUGMENTATIONS,\n",
        "                      \"NUM_AUGS\": NUM_AUGS,\n",
        "                      \"USE_NORMALIZED_CLIP\": USE_NORMALIZED_CLIP,\n",
        "                      \"GRADIENT_CLIPPING\": GRADIENT_CLIPPING,\n",
        "                      \"INITIAL_SEARCH_SIZE\": INITIAL_SEARCH_SIZE}\n",
        "  metadata[\"training\"] = metadata_training\n",
        "  metadata_evolution = {\"POP_SIZE\": POP_SIZE,\n",
        "                        \"EVOLUTION_FREQUENCY\": EVOLUTION_FREQUENCY,\n",
        "                        \"GA_METHOD\": GA_METHOD,\n",
        "                        \"POS_AND_ROT_MUTATION_SCALE\": POS_AND_ROT_MUTATION_SCALE,\n",
        "                        \"SCALE_MUTATION_SCALE\": SCALE_MUTATION_SCALE,\n",
        "                        \"DISTORT_MUTATION_SCALE\": DISTORT_MUTATION_SCALE,\n",
        "                        \"COLOUR_MUTATION_SCALE\": COLOUR_MUTATION_SCALE,\n",
        "                        \"PATCH_MUTATION_PROBABILITY\": PATCH_MUTATION_PROBABILITY,\n",
        "                        \"MAX_MULTIPLE_VISUALISATIONS\": MAX_MULTIPLE_VISUALISATIONS,\n",
        "                        \"USE_EVOLUTION\": USE_EVOLUTION}\n",
        "  metadata[\"evolution\"] = metadata_evolution\n",
        "  metadata_drive = {\"STORE_ON_GOOGLE_DRIVE\": STORE_ON_GOOGLE_DRIVE,\n",
        "                    \"GOOGLE_DRIVE_RESULTS_DIR\": GOOGLE_DRIVE_RESULTS_DIR}\n",
        "  metadata[\"drive\"] = metadata_drive\n",
        "  metadata_patches = {\"PATCH_SET\": PATCH_SET,\n",
        "                      \"URL_TO_PATCH_FILE\": URL_TO_PATCH_FILE,\n",
        "                      \"DRIVE_PATH_TO_PATCH_FILE\": DRIVE_PATH_TO_PATCH_FILE,\n",
        "                      \"NORMALIZE_PATCH_BRIGHTNESS\": NORMALIZE_PATCH_BRIGHTNESS,\n",
        "                      \"FIXED_SCALE_PATCHES\": FIXED_SCALE_PATCHES,\n",
        "                      \"FIXED_SCALE_COEFF\": FIXED_SCALE_COEFF}\n",
        "  metadata[\"patches\"] = metadata_patches\n",
        "\n",
        "  metadata_prompt = {\"PROMPT\": PROMPT,\n",
        "                    \"FILE_BASENAME\": FILE_BASENAME,\n",
        "                    \"VIDEO_STEPS\": VIDEO_STEPS,\n",
        "                    \"TRACE_EVERY\": TRACE_EVERY}\n",
        "  metadata[\"prompt\"] = metadata_prompt\n",
        "\n",
        "  # Write metadata to a Python-interpretable text file.\n",
        "  with open(metadata_filename, \"w\") as f:\n",
        "    for config_key, config in metadata.items():\n",
        "      f.write(f\"# {config_key}\\n\")\n",
        "      for key, value in config.items():\n",
        "        if isinstance(value, str):\n",
        "          f.write(f\"{key} = \\\"{value}\\\"\\n\")\n",
        "        else:\n",
        "          f.write(f\"{key} = {value}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpPJx-r_QZq3"
      },
      "source": [
        "# Training and evolution function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApgzwCVJcU7v",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Image augmentation transformations for training\n",
        "\n",
        "def augmentation_transforms(canvas_width,\n",
        "                            use_normalized_clip=False,\n",
        "                            use_augmentation=False):\n",
        "  \"\"\"Image transforms to produce distorted crops to augment the evaluation.\n",
        "\n",
        "  Args:\n",
        "    canvas_width: width of the drawing canvas\n",
        "    use_normalized_clip: Normalisation to better suit CLIP's training data\n",
        "    use_augmentation: Image augmentation by affine transform\n",
        "\n",
        "  Returns:\n",
        "    transforms\n",
        "  \"\"\"\n",
        "  if use_normalized_clip and use_augmentation:\n",
        "    augment_trans = transforms.Compose(\n",
        "        [transforms.RandomPerspective(fill=1, p=1, distortion_scale=0.6),\n",
        "         transforms.RandomResizedCrop(canvas_width, scale=(0.7, 0.9)),\n",
        "         transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                              (0.26862954, 0.26130258, 0.27577711))])\n",
        "  elif use_augmentation:\n",
        "    augment_trans = transforms.Compose([\n",
        "        transforms.RandomPerspective(fill=1, p=1, distortion_scale=0.6),\n",
        "        transforms.RandomResizedCrop(canvas_width, scale=(0.7, 0.9)),\n",
        "    ])\n",
        "  elif use_normalized_clip:\n",
        "    augment_trans = transforms.Normalize(\n",
        "        (0.48145466, 0.4578275, 0.40821073),\n",
        "        (0.26862954, 0.26130258, 0.27577711))\n",
        "  else:\n",
        "    augment_trans = transforms.RandomPerspective(\n",
        "        fill=1, p=0, distortion_scale=0)\n",
        "\n",
        "  return augment_trans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4XIVMSJuWgxG"
      },
      "outputs": [],
      "source": [
        "#@title Training functions\n",
        "\n",
        "def moving_average(a, n=3) :\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "\n",
        "def plot_and_save_losses(loss_history, title=\"Losses\", filename=None):\n",
        "  losses = np.array(loss_history)\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.xlabel(\"Training steps\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(title)\n",
        "  plt.plot(moving_average(losses, n=3))\n",
        "  if filename:\n",
        "    np.save(filename + \".npy\", losses, allow_pickle=True)\n",
        "    plt.savefig(filename + \".png\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def make_optimizer(generator, learning_rate):\n",
        "  \"\"\"Make optimizer for generator's parameters.\n",
        "\n",
        "  Args:\n",
        "    generator: generator model\n",
        "    learning_rate: learning rate\n",
        "    input_learing_rate: learning rate for input\n",
        "\n",
        "  Returns:\n",
        "    optimizer\n",
        "  \"\"\"\n",
        "\n",
        "  my_list = ['positions_top']\n",
        "  params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in my_list, \n",
        "                                               generator.named_parameters()))))\n",
        "  base_params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] not in \n",
        "                                      my_list, generator.named_parameters()))))\n",
        "  lr_scheduler = torch.optim.SGD([{'params': base_params},\n",
        "                                  {'params': params, 'lr': learning_rate}],\n",
        "                                    lr=learning_rate)\n",
        "  return lr_scheduler\n",
        "\n",
        "\n",
        "def text_features(prompts):\n",
        "  # Compute CLIP features for all prompts.\n",
        "  text_inputs = []\n",
        "  for prompt in prompts:\n",
        "    text_inputs.append(clip.tokenize(prompt).to(device))\n",
        "\n",
        "  features = []\n",
        "  with torch.no_grad():\n",
        "    for text_input in text_inputs:\n",
        "      features.append(clip_model.encode_text(text_input))\n",
        "  return features\n",
        "\n",
        "\n",
        "def create_augmented_batch(images, augment_trans, text_features):\n",
        "  \"\"\"Create batch of images to be evaluated.\n",
        "  \n",
        "  Returns:\n",
        "    img_batch: For compositional images the batch contains all the regions.\n",
        "        Otherwise the batch contains augmented versions of the original images.\n",
        "    num_augs: number of images per original image\n",
        "    expanded_text_features: a text feature for each augmentation\n",
        "    loss_weights: weights for loss associated with each augmentation image\n",
        "  \"\"\"\n",
        "  images = images.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "  expanded_text_features = []\n",
        "  if USE_IMAGE_AUGMENTATIONS:\n",
        "    num_augs = NUM_AUGS\n",
        "    img_augs = [] \n",
        "    for n in range(NUM_AUGS):\n",
        "      img_n = augment_trans(images)\n",
        "      img_augs.append(img_n)\n",
        "      expanded_text_features.append(text_features[0])\n",
        "    img_batch = torch.cat(img_augs)\n",
        "    # Given images [P0, P1] and augmentations [a0(), a1()], output format:\n",
        "    # [a0(P0), a0(P1), a1(P0), a1(P1)]\n",
        "  else:\n",
        "    num_augs = 1\n",
        "    if USE_NORMALIZED_CLIP:\n",
        "      img_batch = augment_trans(images)\n",
        "    else:\n",
        "      img_batch = images\n",
        "    expanded_text_features.append(text_features[0])\n",
        "  return img_batch, num_augs, expanded_text_features, [1] * NUM_AUGS\n",
        "\n",
        "\n",
        "def create_compositional_batch(images, augment_trans, text_features):\n",
        "  \"\"\"Create 10 sub-images per image by augmenting each with 3x3 crops.\n",
        "\n",
        "  Args:\n",
        "    images: population of N images, format [N, C, H, W]\n",
        "\n",
        "  Returns:\n",
        "    Tensor of all compositional sub-images + originals; [N*10, C, H, W] format:\n",
        "        [x0_y0(P0) ... x0_y0(PN), ..., x2_y2(P0) ... x2_y2(PN), P0, ..., PN]\n",
        "    10: Number of sub-images + whole, per original image.\n",
        "    expanded_text_features: list of text features, 1 for each composition image\n",
        "    loss_weights: weights for the losses corresponding to each composition image\n",
        "    \"\"\"\n",
        "  if len(text_features) != 10:\n",
        "    # text_features should already be 10 in size.\n",
        "    raise ValueError(\n",
        "        \"10 text prompts required for compositional image creation\")\n",
        "  resize_for_clip = transforms.Compose([transforms.Scale((224,224))])\n",
        "  img_swap = torch.swapaxes(images, 3, 1)\n",
        "  ims = []\n",
        "  i = 0\n",
        "  for x in range(3):\n",
        "    for y in range(3):\n",
        "      #  print(f\"Prompt for x={x}, y={y} is {PROMPTS[i]}\")\n",
        "      for k in range(images.shape[0]):\n",
        "        ims.append(resize_for_clip(\n",
        "            img_swap[k][:, y * 112 : y * 112 + 224, x * 112 : x * 112 + 224]))  \n",
        "      i += 1\n",
        "  \n",
        "  # Top-level (whole) images\n",
        "  for k in range(images.shape[0]):\n",
        "    ims.append(resize_for_clip(img_swap[k]))\n",
        "  all_img = torch.stack(ims)\n",
        "  all_img = torch.swapaxes(all_img, 1, 3)\n",
        "  all_img = all_img.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "  all_img = augment_trans(all_img)\n",
        "\n",
        "  # Last image gets 9 times as much weight\n",
        "  common_weight = 1 / 5\n",
        "  loss_weights = [common_weight] * 9\n",
        "  loss_weights.append(9 * common_weight)\n",
        "  return all_img, 10, text_features, loss_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFko54cGSkai",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Evaluation and step of optimization\n",
        "\n",
        "# Show each image being evaluated for debugging purposes.\n",
        "VISUALISE_BATCH_IMAGES = False\n",
        "\n",
        "def evaluation(t, clip_enc, generator, augment_trans, text_features,\n",
        "               compositional_image, prompts):\n",
        "  \"\"\"Do a step of evaluation, returning images and losses.\n",
        "\n",
        "  Args:\n",
        "    t: step count\n",
        "    clip_enc: model for CLIP encoding\n",
        "    generator: drawing generator to optimise\n",
        "    augment_trans: transforms for image augmentation\n",
        "    text_features: tuple with the prompt two negative prompts\n",
        "    compositional_image: use multiple CLIPs and prompts\n",
        "    prompts: for debugging/visualisation - the list of text prompts\n",
        "  \n",
        "  Returns:\n",
        "    loss: torch.Tensor of single combines loss\n",
        "    losses_separate_np: numpy array of loss for each image\n",
        "    losses_individuals_np: numpy array with loss for each population individual\n",
        "    img_np: numpy array of images from the generator\n",
        "  \"\"\"\n",
        "\n",
        "  # Annealing parameters.\n",
        "  params = {'gamma': t / OPTIM_STEPS}\n",
        "\n",
        "  # Rebuild the generator.\n",
        "  img = generator(params)\n",
        "  img_np = img.detach().cpu().numpy()\n",
        "\n",
        "  # Create images for different regions\n",
        "  pop_size = img.shape[0]\n",
        "  if compositional_image:\n",
        "    (img_batch, num_augs, text_features, loss_weights\n",
        "     ) = create_compositional_batch(img, augment_trans, text_features)\n",
        "  else:\n",
        "    (img_batch, num_augs, text_features, loss_weights\n",
        "     ) = create_augmented_batch(img, augment_trans, text_features)\n",
        "  losses = torch.zeros(pop_size, num_augs).to(device)\n",
        "\n",
        "  # Compute and add losses after augmenting the image with transforms.\n",
        "  img_batch = torch.clip(img_batch, 0, 1)  # clip the images.\n",
        "  image_features = clip_enc.encode_image(img_batch)\n",
        "  count = 0\n",
        "  for n in range(num_augs):  # number of augmentations or composition images\n",
        "    for p in range(pop_size):\n",
        "      loss = torch.cosine_similarity(\n",
        "          text_features[n], image_features[count:count+1], dim=1\n",
        "          )[0] * loss_weights[n]\n",
        "      losses[p, n] -= loss\n",
        "      if VISUALISE_BATCH_IMAGES and t % 500 == 0:\n",
        "        # Show all the images in the batch along with their losses.\n",
        "        if compositional_image:\n",
        "          print(f\"Loss {loss} for image region with prompt {prompts[n]}:\")\n",
        "        else:\n",
        "          print(f\"Loss {loss} for image augmentation with prompt {prompts[0]}:\")\n",
        "        show_and_save(img_batch[count].unsqueeze(0), img_format=\"SCHW\")\n",
        "      count += 1\n",
        "  loss = torch.sum(losses) / pop_size\n",
        "  losses_separate_np = losses.detach().cpu().numpy()\n",
        "  # Sum losses for all each population individual.\n",
        "  losses_individuals_np = losses_separate_np.sum(axis=1)\n",
        "  return loss, losses_separate_np, losses_individuals_np, img_np\n",
        "\n",
        "\n",
        "def step_optimization(t, clip_enc, lr_scheduler, generator, augment_trans,\n",
        "                      text_features, compositional_image, prompts,\n",
        "                      final_step=False):\n",
        "  \"\"\"Do a step of optimization.\n",
        "\n",
        "  Args:\n",
        "    t: step count\n",
        "    clip_enc: model for CLIP encoding\n",
        "    lr_scheduler: optimizer\n",
        "    generator: drawing generator to optimise\n",
        "    augment_trans: transforms for image augmentation\n",
        "    text_features: list or 1 or 9 prompts for normal and compositional creation\n",
        "    compositional_image: use multiple CLIPs and prompts\n",
        "    prompts: for debugging/visualisation - the list of text prompts\n",
        "    final_step: if True does extras such as saving the model\n",
        "\n",
        "  Returns:\n",
        "    losses_np: numpy array with loss for each population individual\n",
        "    losses_separate_np: numpy array of loss for each image\n",
        "  \"\"\"\n",
        "\n",
        "  # Anneal learning rate and other parameters.\n",
        "  if t == int(OPTIM_STEPS / 3):\n",
        "    for g in lr_scheduler.param_groups:\n",
        "      g[\"lr\"] = g[\"lr\"] / 2.0\n",
        "  if t == int(OPTIM_STEPS * (2/3)):\n",
        "    for g in lr_scheduler.param_groups:\n",
        "      g[\"lr\"] = g[\"lr\"] / 2.0\n",
        "  params = {'gamma': t / OPTIM_STEPS}\n",
        "\n",
        "  # Forward pass. \n",
        "  lr_scheduler.zero_grad()\n",
        "  loss, losses_separate_np, losses_np, img_np = evaluation(\n",
        "      t, clip_enc, generator, augment_trans, text_features, compositional_image,\n",
        "      prompts)\n",
        "\n",
        "  # Backpropagate the gradients.\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm(generator.parameters(), GRADIENT_CLIPPING)\n",
        "  \n",
        "  # Decay the learning rate.\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  # Render the big version.\n",
        "  if final_step:\n",
        "    show_and_save(img_np, t=t, img_format=\"SHWC\")\n",
        "    print(\"Saving model...\")\n",
        "    torch.save(generator.state_dict(), DIR_RESULTS + \"/generator.pt\")\n",
        "\n",
        "  if t % TRACE_EVERY == 0:\n",
        "    show_and_save(img_np, \n",
        "                  max_display=MAX_MULTIPLE_VISUALISATIONS,\n",
        "                  stitch=True, img_format=\"SHWC\")\n",
        "\n",
        "    print(\"Iteration {:3d}, rendering loss {:.6f}\".format(t, loss.item()))\n",
        "  return losses_np, losses_separate_np, img_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XlcTY0mjmQ5W"
      },
      "outputs": [],
      "source": [
        "#@title Evolution functions\n",
        "\n",
        "def population_evolution_step(generator, losses):\n",
        "  \"\"\"GA for the population.\"\"\"\n",
        "  if GA_METHOD == \"Microbial\":\n",
        "    # Competition between 2 random individuals; mutated winner replaces loser.\n",
        "    indices = list(range(len(losses)))\n",
        "    random.shuffle(indices)\n",
        "    select_1, select_2 = indices[0], indices[1]\n",
        "    if losses[select_1] < losses[select_2]:\n",
        "      # print(f\"Replacing {select_2} with {select_1}\")\n",
        "      generator.copy_and_mutate_s(select_1, select_2)\n",
        "    else:\n",
        "      # print(f\"Replacing {select_1} with {select_2}\")\n",
        "      generator.copy_and_mutate_s(select_2, select_1)\n",
        "  elif GA_METHOD == \"Evolutionary Strategies\":\n",
        "    # Replace rest of population with mutants of the best.\"\"\"\n",
        "    winner = np.argmin(losses)\n",
        "    for other in range(len(losses)):\n",
        "      if other == winner:\n",
        "        continue\n",
        "      generator.copy_and_mutate_s(winner, other)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collage-making class definitions"
      ],
      "metadata": {
        "id": "y8gtKaa8kGLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CollageMaker class\n",
        "\n",
        "class CollageMaker():\n",
        "  def __init__(self, prompts, \n",
        "               segmented_data, \n",
        "               background_image,\n",
        "               compositional_image,\n",
        "               output_dir,\n",
        "               file_basename, \n",
        "               video_steps,\n",
        "               population_video):\n",
        "    self._prompts = prompts\n",
        "    self._segmented_data = segmented_data\n",
        "    self._background_image = background_image\n",
        "    self._compositional_image = compositional_image\n",
        "    self._file_basename = file_basename\n",
        "    self._output_dir = output_dir\n",
        "    self._population_video = population_video\n",
        "\n",
        "    self._video_steps = video_steps\n",
        "    if self._video_steps:\n",
        "      self._video_writer = VideoWriter(\n",
        "          filename=f\"{self._output_dir}/{self._file_basename}.mp4\")\n",
        "      if self._population_video:\n",
        "        self._population_video_writer = VideoWriter(\n",
        "            filename=f\"{self._output_dir}/{self._file_basename}_pop_sample.mp4\")\n",
        "    \n",
        "    if self._compositional_image:\n",
        "      if len(self._prompts) != 10:\n",
        "        raise ValueError(\n",
        "            \"Missing compositional image prompts; found {len(self._prompts)}\")\n",
        "      print(\"Global prompt is\", self._prompts[-1])\n",
        "      print(\"Composition prompts\", self._prompts)\n",
        "    else:\n",
        "      if len(self._prompts) != 1:\n",
        "        raise ValueError(\n",
        "            \"Missing compositional image prompts; found {len(self._prompts)}\")\n",
        "      print(\"CLIP prompt\", self._prompts[0])\n",
        "    \n",
        "    # Prompt to CLIP features.\n",
        "    self._prompt_features = text_features(self._prompts)\n",
        "    self._augmentations = augmentation_transforms(\n",
        "        224,\n",
        "        use_normalized_clip=USE_NORMALIZED_CLIP,\n",
        "        use_augmentation=USE_IMAGE_AUGMENTATIONS)\n",
        "    \n",
        "    # Create population of collage generators.\n",
        "    self._generator = PopulationCollage(\n",
        "        is_high_res=False,\n",
        "        pop_size=POP_SIZE,\n",
        "        segmented_data=segmented_data,\n",
        "        background_image=background_image)\n",
        "    \n",
        "    # Initial search over hyper-parameters.\n",
        "    if INITIAL_SEARCH_SIZE > 1:\n",
        "      print(f'\\nInitial random search over {INITIAL_SEARCH_SIZE} individuals')\n",
        "      for j in range(POP_SIZE):\n",
        "        generator_search = PopulationCollage(\n",
        "            is_high_res=False,\n",
        "            pop_size=INITIAL_SEARCH_SIZE,\n",
        "            segmented_data=segmented_data,\n",
        "            background_image=background_image,\n",
        "            compositional_image=self._compositional_image)\n",
        "        _, _, losses, _ = evaluation(\n",
        "            0, clip_model, generator_search, augmentations, prompt_features)\n",
        "        print(f\"Search {losses}\")\n",
        "        idx_best = np.argmin(losses)\n",
        "        generator.copy_from(generator_search, j, idx_best)\n",
        "        del generator_search\n",
        "      print(f'Initial random search done\\n')\n",
        "    \n",
        "    self._optimizer = make_optimizer(self._generator, LEARNING_RATE)\n",
        "    self._step = 0\n",
        "    self._losses_history = []\n",
        "    self._losses_separated_history = []\n",
        "\n",
        "  @property\n",
        "  def generator(self):\n",
        "    return self._generator\n",
        "    \n",
        "  @property\n",
        "  def step(self):\n",
        "    return self._step\n",
        "\n",
        "  def loop(self):\n",
        "    \"\"\"Main optimisation/image generation loop. Can be interrupted.\"\"\"\n",
        "    if self._step == 0:\n",
        "      print('Starting optimization of collage.')\n",
        "    else:\n",
        "      print(f'Continuing optimization of collage at step {self._step}.')\n",
        "      if self._video_steps:\n",
        "        print(f\"Aborting video creation (does not work when interrupted).\")\n",
        "        self._video_steps = 0\n",
        "        self._video_writer = None\n",
        "        if self._population_video_writer:\n",
        "          self._population_video_writer = None\n",
        "    \n",
        "    while self._step < OPTIM_STEPS:\n",
        "      last_step = self._step == (OPTIM_STEPS - 1)\n",
        "      losses, losses_separated, img_batch = step_optimization(\n",
        "          self._step, clip_model, self._optimizer, self._generator,\n",
        "          self._augmentations, self._prompt_features, \n",
        "          self._compositional_image, self._prompts, final_step=last_step)\n",
        "      self._add_video_frames(img_batch, losses)\n",
        "      self._losses_history.append(losses)\n",
        "      self._losses_separated_history.append(losses_separated)\n",
        "    \n",
        "      if USE_EVOLUTION and self._step and self._step % EVOLUTION_FREQUENCY == 0:\n",
        "        population_evolution_step(self._generator, losses) \n",
        "      self._step += 1\n",
        "    \n",
        "\n",
        "  def high_res_render(self, \n",
        "                      segmented_data_high_res, \n",
        "                      background_image_high_res,\n",
        "                      gamma=1.0, \n",
        "                      show=True,\n",
        "                      save=True):\n",
        "    \"\"\"Save and/or show a high res render using high-res patches.\"\"\"\n",
        "    generator = PopulationCollage(\n",
        "        is_high_res=True,\n",
        "        pop_size=1,\n",
        "        segmented_data=segmented_data_high_res,\n",
        "        background_image=background_image_high_res)\n",
        "    idx_best = np.argmin(self._losses_history[-1])\n",
        "    print(f'Lowest loss for indices: {idx_best}')\n",
        "    generator.copy_from(self._generator, 0, idx_best)\n",
        "    # Show high res version given a generator\n",
        "    generator_cpu = copy.deepcopy(generator)\n",
        "    generator_cpu = generator_cpu.to('cpu')\n",
        "    generator_cpu.tensors_to('cpu')\n",
        "  \n",
        "    params = {'gamma': gamma}\n",
        "    with torch.no_grad():\n",
        "      img_high_res = generator_cpu.forward(params)\n",
        "    img = img_high_res.detach().cpu().numpy()[0]\n",
        "  \n",
        "    img = np.clip(img, 0.0, 1.0)\n",
        "    if save or show:\n",
        "      # Swap Red with Blue\n",
        "      img = img[...,[2, 1, 0]]  \n",
        "      img = np.clip(img, 0.0, 1.0) * 255\n",
        "    if save:\n",
        "      image_filename = f\"{self._output_dir}/{self._file_basename}.png\"\n",
        "      cv2.imwrite(image_filename, img)\n",
        "    if show:\n",
        "      cv2_imshow(img)\n",
        "      cv2.waitKey()\n",
        "    return img\n",
        "\n",
        "  def finish(self):\n",
        "    \"\"\"Finish video writing and save all other data.\"\"\"\n",
        "    if self._losses_history:\n",
        "      losses_filename = f\"{self._output_dir}/{self._file_basename}_losses\"\n",
        "      plot_and_save_losses(self._losses_history, \n",
        "                           title=f\"{self._file_basename} Losses\",\n",
        "                           filename=losses_filename)\n",
        "    if self._video_steps:\n",
        "      self._video_writer.close()\n",
        "    if self._population_video:\n",
        "      self._population_video_writer.close()\n",
        "    metadata_filename = f\"{self._output_dir}/{self._file_basename}_metadata.py\"\n",
        "    export_metadata(metadata_filename)\n",
        "\n",
        "  def _add_video_frames(self, img_batch, losses):\n",
        "    \"\"\"Add images from numpy image batch to video writers.\n",
        "    \n",
        "    Args:\n",
        "      img_batch: numpy array, batch of images (S,H,W,C)\n",
        "      losses: numpy array, losses for each generator (S,N)\n",
        "    \"\"\"\n",
        "    if self._video_steps and self._step % self._video_steps == 0:\n",
        "      # Write image to video.\n",
        "      best_img = img_batch[np.argmin(losses)]\n",
        "      self._video_writer.add(cv2.resize(\n",
        "          best_img, (best_img.shape[1] * 3, best_img.shape[0] * 3)))\n",
        "      if self._population_video:\n",
        "        laid_out = layout_img_batch(img_batch)\n",
        "        self._population_video_writer.add(cv2.resize(\n",
        "            laid_out, (laid_out.shape[1] * 2, laid_out.shape[0] * 2)))"
      ],
      "metadata": {
        "id": "oHGr2ogDRBMt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CollageTiler class\n",
        "\n",
        "class CollageTiler():\n",
        "  def __init__(self,\n",
        "               wide, high,\n",
        "               prompts,\n",
        "               segmented_data, \n",
        "               segmented_data_high_res, \n",
        "               fixed_background_image,\n",
        "               background_use,\n",
        "               compositional,\n",
        "               high_res_multiplier,\n",
        "               output_dir,\n",
        "               file_basename,\n",
        "               video_steps=0):\n",
        "    \"\"\"Creates a large collage by producing multiple interlaced collages.\n",
        "\n",
        "    Args:\n",
        "      width: number of tiles wide\n",
        "      height: number of tiles high\n",
        "      prompts: list of prompts for the collage maker\n",
        "      segmented_data: patch data for collage maker to use during opmtimisation\n",
        "      segmented_data_high_res: high res patch data for final renders\n",
        "      fixed_background_image: highest res background image\n",
        "      background_use: how to use the background, e.g. per tile or whole image\n",
        "      compositional: bool, use compositional for multi-CLIP collage tiles\n",
        "      output_dir: directory for generated files\n",
        "      file_basename: base name for files\n",
        "      video_steps: How often to capture frames for videos. Zero=never\n",
        "    \"\"\"\n",
        "    self._tiles_wide = wide\n",
        "    self._tiles_high = high\n",
        "    self._prompts = prompts\n",
        "    self._segmented_data = segmented_data\n",
        "    self._segmented_data_high_res = segmented_data_high_res\n",
        "    self._fixed_background_image = fixed_background_image\n",
        "    self._background_use = background_use\n",
        "    self._compositional_image = compositional\n",
        "    self._high_res_multiplier = high_res_multiplier\n",
        "    self._output_dir = output_dir\n",
        "    self._file_basename = file_basename\n",
        "    self._video_steps = video_steps\n",
        "\n",
        "    self._tile_base = \"img_tile_y{}_x{}.npy\"\n",
        "    self._tile_width = 448 if self._compositional_image else 224\n",
        "    self._tile_height = 448 if self._compositional_image else 224\n",
        "    self._overlap = 1. / 3.\n",
        "\n",
        "    # Size of bigger image\n",
        "    self._width = int(((2 * self._tiles_wide + 1) * self._tile_width) / 3.)\n",
        "    self._height = int(((2 * self._tiles_high + 1) * self._tile_height) / 3.)\n",
        "\n",
        "    self._high_res_tile_width = self._tile_width * self._high_res_multiplier\n",
        "    self._high_res_tile_height = self._tile_height * self._high_res_multiplier\n",
        "\n",
        "    self._print_info()\n",
        "    self._x = 0\n",
        "    self._y = 0\n",
        "    self._collage_maker = None\n",
        "    self._fixed_background = self._scale_fixed_background(high_res=True)\n",
        "\n",
        "  def _print_info(self):\n",
        "    print(f\"Tiling {self._tiles_wide}x{self._tiles_high} collages\")\n",
        "    print(\"Optimisation:\")\n",
        "    print(f\"Tile size: {self._tile_width}x{self._tile_height}\")\n",
        "    print(f\"Global size: {self._width}x{self._height} (WxH)\")\n",
        "    print(\"High res:\")\n",
        "    print(\n",
        "        f\"Tile size: {self._high_res_tile_width}x{self._high_res_tile_height}\")\n",
        "    print(f\"Global size: {self._width * self._high_res_multiplier}x\"\n",
        "          f\"{self._height * self._high_res_multiplier} (WxH)\")\n",
        "    print(self._prompts)\n",
        "    for i, tile_prompts in enumerate(self._prompts):\n",
        "      print(f\"Tile {i} prompts: {tile_prompts}\")\n",
        "\n",
        "  def loop(self):\n",
        "    while self._y < self._tiles_high:\n",
        "      while self._x < self._tiles_wide:\n",
        "        if not self._collage_maker:\n",
        "          # Create new collage maker with its unique background.\n",
        "          print(f\"New collage creator for y{self._y}, x{self._x} with bg:\")\n",
        "          tile_bg, self._tile_high_res_bg = self._get_tile_background()\n",
        "          tile_name = f\"{self._file_basename}_y{self._y}_x{self._x}\"\n",
        "          show_and_save(tile_bg, img_format=\"SCHW\", stitch=False)\n",
        "          prompts = self._prompts[self._y * self._tiles_wide + self._x]\n",
        "          self._collage_maker = CollageMaker(\n",
        "              prompts, self._segmented_data, \n",
        "              tile_bg, self._compositional_image, self._output_dir,\n",
        "              tile_name, self._video_steps, population_video=False)\n",
        "        self._collage_maker.loop()\n",
        "        collage_img = self._collage_maker.high_res_render(\n",
        "            self._segmented_data_high_res, \n",
        "            self._tile_high_res_bg,\n",
        "            gamma=1.0, \n",
        "            show=True,\n",
        "            save=True)\n",
        "        self._save_tile(collage_img / 255)\n",
        "        # TODO: Currently calling finish will save video and download zip which is not needed.\n",
        "        # self._collage_maker.finish()\n",
        "        del self._collage_maker\n",
        "        self._collage_maker = None\n",
        "        self._x += 1\n",
        "      self._y += 1\n",
        "      self._x = 0\n",
        "    return collage_img  # SHWC\n",
        "\n",
        "  def _save_tile(self, img):\n",
        "    filename = f\"final_image_part{str(self._y)}_{str(self._x)}.npy\"\n",
        "    np.save(filename, img)\n",
        "    background_image_np = np.asarray(img)\n",
        "    background_image_np = background_image_np[..., ::-1].copy()\n",
        "    np.save(self._tile_base.format(self._y, self._x), background_image_np)\n",
        "\n",
        "  def _scale_fixed_background(self, high_res=True):\n",
        "    if self._fixed_background_image is None:\n",
        "      return None\n",
        "    multiplier = self._high_res_multiplier if high_res else 1\n",
        "    if self._background_use == \"Local\":\n",
        "      height = self._tile_height * multiplier\n",
        "      width = self._tile_width * multiplier\n",
        "    elif self._background_use == \"Global\":\n",
        "      height = self._height * multiplier\n",
        "      width = self._width * multiplier\n",
        "    return resize(self._fixed_background_image.astype(float), (height, width))\n",
        "\n",
        "  def _get_tile_background(self):\n",
        "    \"\"\"Get the background for a particular tile.\n",
        "  \n",
        "    This involves getting bordering imagery from left, top left, above and top \n",
        "    right, where appropriate.\n",
        "    i.e. tile (1,1) shares overlap with (0,1), (0,2) and (1,0)\n",
        "    (0,0), (0,1), (0,2), (0,3)\n",
        "    (1,0), (1,1), (1,2), (1,3)\n",
        "    (2,0), (2,1), (2,2), (2,3)\n",
        "  \n",
        "    Note that (0,0) is not needed as its contribution is already in (0,1) \n",
        "    \"\"\"\n",
        "    if self._fixed_background is None:\n",
        "      tile_border_bg = np.zeros((self._high_res_tile_height,\n",
        "                                self._high_res_tile_width, 3))\n",
        "    else:\n",
        "      if self._background_use == \"Local\":\n",
        "        tile_border_bg = self._fixed_background.copy()\n",
        "      else:  # Crop out section for this tile.\n",
        "        orgin_y = self._y * (self._high_res_tile_height\n",
        "                             - math.ceil(self._tile_height * self._overlap) \n",
        "                             * self._high_res_multiplier)\n",
        "        orgin_x = self._x * (self._high_res_tile_width \n",
        "                             - math.ceil(self._tile_width * self._overlap) \n",
        "                             * self._high_res_multiplier)\n",
        "        tile_border_bg = self._fixed_background[\n",
        "            orgin_y : orgin_y + self._high_res_tile_height,\n",
        "            orgin_x : orgin_x + self._high_res_tile_width, :]\n",
        "    tile_idx = dict()\n",
        "    if self._x > 0:\n",
        "      tile_idx[\"left\"] = (self._y, self._x - 1)\n",
        "    if self._y > 0:\n",
        "      tile_idx[\"above\"] = (self._y - 1, self._x)\n",
        "      if self._x < self._tiles_wide - 1:  # Penultimate on the row\n",
        "        tile_idx[\"above_right\"] = (self._y - 1, self._x + 1)\n",
        "\n",
        "    # Get and insert bodering tile content in this order.\n",
        "    if \"above\" in tile_idx:\n",
        "      self._copy_overlap(tile_border_bg, \"above\", tile_idx[\"above\"])\n",
        "    if \"above_right\" in tile_idx:\n",
        "      self._copy_overlap(tile_border_bg, \"above_right\", tile_idx[\"above_right\"])\n",
        "    if \"left\" in tile_idx:\n",
        "      self._copy_overlap(tile_border_bg, \"left\", tile_idx[\"left\"])\n",
        "\n",
        "    background_image = self._resize_image_for_torch(\n",
        "        tile_border_bg, self._tile_height, self._tile_width)\n",
        "    background_image_high_res = self._resize_image_for_torch(\n",
        "        tile_border_bg,\n",
        "        self._high_res_tile_height,\n",
        "        self._high_res_tile_width).to('cpu')\n",
        "  \n",
        "    return background_image, background_image_high_res\n",
        "\n",
        "  def _resize_image_for_torch(self, img, height, width):\n",
        "    # Resize and permute to format used by Collage class (SCHW).\n",
        "    img = torch.tensor(resize(img.astype(float),\n",
        "                              (height, width))).cuda()\n",
        "    return img.permute(2, 0, 1).to(torch.float32)\n",
        "\n",
        "  def _copy_overlap(self, target, location, tile_idx):\n",
        "    # print(\n",
        "    #     f\"Copying overlap from {location} ({tile_idx}) for {self._y},{self._x}\")\n",
        "    big_height = self._high_res_tile_height\n",
        "    big_width = self._high_res_tile_width\n",
        "    pixel_overlap = int(big_width * self._overlap)\n",
        "\n",
        "    # print(f\"Loading tile {self._tile_base.format(tile_idx[0], tile_idx[1])}\")\n",
        "    source = np.load(self._tile_base.format(tile_idx[0], tile_idx[1]))\n",
        "    if location == \"above\":\n",
        "      import pdb; pdb.set_trace()\n",
        "      target[0 : pixel_overlap, 0 : big_width, :] = source[\n",
        "          big_height - pixel_overlap : big_height, 0 : big_width, :]\n",
        "    if location == \"left\":\n",
        "      target[:, 0 : pixel_overlap, :] = source[\n",
        "          :, big_width - pixel_overlap : big_width, :]\n",
        "    elif location == \"above_right\":\n",
        "      target[0 : pixel_overlap, big_width - pixel_overlap : big_width, :] = source[\n",
        "          big_height - pixel_overlap : big_height, 0 : pixel_overlap, :]\n",
        "\n",
        "  def assemble_tiles(self):\n",
        "    # Stitch together the whole image.\n",
        "    big_height = self._high_res_tile_height\n",
        "    big_width = self._high_res_tile_width\n",
        "    full_height = int((big_height + 2 * big_height * self._tiles_high) / 3)\n",
        "    full_width = int((big_width + 2 * big_width * self._tiles_wide) / 3)\n",
        "    full_image = np.zeros((full_height, full_width, 3)).astype('float32')\n",
        "    \n",
        "    for y in range(self._tiles_high):\n",
        "      for x in range(self._tiles_wide):\n",
        "          tile = np.load(self._tile_base.format(y, x))\n",
        "          y_offset = int(big_height * y * 2 / 3)\n",
        "          x_offset = int(big_width * x * 2 / 3)\n",
        "          full_image[y_offset : y_offset + big_height,\n",
        "                     x_offset : x_offset + big_width, :] = tile[:, :, :]\n",
        "    show_and_save(\n",
        "        full_image, img_format=\"SHWC\", stitch=False, filename=\"full_image.png\")"
      ],
      "metadata": {
        "id": "9QXc_I3kkUx0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyyw6Glris_e"
      },
      "source": [
        "# Make Collages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6drbSbshNN0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Configure image prompt and content\n",
        "\n",
        "#@markdown Enter a **global** description of the image, e.g. 'a photorealistic chicken'\n",
        "# PROMPT = \"A photorealistic chicken\"  #@param {type:\"string\"}\n",
        "\n",
        "GLOBAL_PROMPT = \"color photograph of a tropical street with brightly colored houses\"   #@param {type:\"string\"}\n",
        "\n",
        "FILE_BASENAME = \"dancers\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Configure a background, e.g. uploaded picture or solid colour.\n",
        "# NOTE!! Check code in the rest of this cell if modifying these text strings.\n",
        "BACKGROUND = \"Load image from URL\" #@param [\"None (black)\", \"Solid colour below\", \"Upload image to Colab\", \"Load image from URL\"]\n",
        "# BACKGROUND = \"Load image from Google Drive\" #@param [\"None (black)\", \"Solid colour below\", \"Upload image to Colab\", \"Load image from URL\", \"Load image from Google Drive\"]\n",
        "#@markdown Background usage: Global = use image across whole image; Local = reuse same image for every tile\n",
        "BACKGROUND_USE = \"Global\" #@param [\"Global\", \"Local\"]\n",
        "\n",
        "#@markdown Colour configuration for solid colour background\n",
        "BACKGROUND_RED = 195 #@param {type:\"slider\", min:0, max:255, step:1}\n",
        "BACKGROUND_GREEN = 181 #@param {type:\"slider\", min:0, max:255, step:1}\n",
        "BACKGROUND_BLUE = 172 #@param {type:\"slider\", min:0, max:255, step:1}\n",
        "\n",
        "#@markdown URL if downloading image file from website:\n",
        "BACKGROUND_IMAGE_URL = \"https://2ne1.com/a/biggest_chicken_ever.jpg\" #@param {type:\"string\"}\n",
        "#markdown Path if loading image file from Google Drive:\n",
        "# BACKGROUND_IMAGE_DRIVE_PATH = \"Art/Collage/Backgrounds/biggest_chicken_ever.jpg\" #@param {type:\"string\"}\n",
        "#@markdown Debugging and monitoring\n",
        "\n",
        "VIDEO_STEPS =   500#@param {type:\"integer\"}\n",
        "TRACE_EVERY =   500#@param {type:\"integer\"}\n",
        "\n",
        "PROMPTS = [GLOBAL_PROMPT]\n",
        "\n",
        "background_image = None\n",
        "\n",
        "def upload_files():\n",
        "  # Upload and save to Colab's disk.\n",
        "  uploaded = files.upload()\n",
        "  # Save to disk\n",
        "  for k, v in uploaded.items():\n",
        "    open(k, 'wb').write(v)\n",
        "  return list(uploaded.keys())\n",
        "\n",
        "def load_image(filename, as_cv2_image=False, show=False):\n",
        "  # Load an image as [0,1] RGB numpy array or cv2 image format.\n",
        "  img = cv2.imread(filename)\n",
        "  if show:\n",
        "    cv2_imshow(img)\n",
        "  if as_cv2_image:\n",
        "    return img  # With colour format BGR\n",
        "  img = np.asarray(img)\n",
        "  return img[..., ::-1] / 255.  # Reverse colour dim to convert BGR to RGB\n",
        "\n",
        "if BACKGROUND == \"None (black)\":\n",
        "  # 'No background' is actually a black background.\n",
        "  BACKGROUND = \"Solid colour below\"\n",
        "  BACKGROUND_RED = 0 \n",
        "  BACKGROUND_GREEN = 0\n",
        "  BACKGROUND_BLUE = 0\n",
        "\n",
        "if BACKGROUND == \"Load image from URL\":\n",
        "  background_image = cached_url_download(BACKGROUND_IMAGE_URL,\n",
        "                                         format=\"image as RGB\")\n",
        "elif BACKGROUND == \"Solid colour below\":\n",
        "  background_image = np.ones((10, 10, 3), dtype=np.float32)\n",
        "  background_image[:, :, 0] = BACKGROUND_RED\n",
        "  background_image[:, :, 1] = BACKGROUND_GREEN\n",
        "  background_image[:, :, 2] = BACKGROUND_BLUE\n",
        "  background_image /= 255.\n",
        "  print('Defined background colour ({}, {}, {})'.format(\n",
        "      BACKGROUND_RED, BACKGROUND_GREEN, BACKGROUND_BLUE))\n",
        "elif BACKGROUND == \"Load image from Google Drive\":\n",
        "  drive.mount(MOUNT_DIR)\n",
        "  data_file = pathlib.PurePath(MOUNT_DIR, \"MyDrive\", \n",
        "                               BACKGROUND_IMAGE_DRIVE_PATH)\n",
        "  print(\"Reading\", data_file)\n",
        "  background_image = load_image(data_file)\n",
        "else:  # \"Upload image to Colab\"\n",
        "  backgrounds = upload_files()\n",
        "  background_image = load_image(backgrounds[0], show=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tile prompts and tiling settings\n",
        "\n",
        "TILE_IMAGES = True #@param {type:\"boolean\"}\n",
        "\n",
        "TILES_WIDE = 2  #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "TILES_HIGH = 2  #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "# Turn off tiling if either boolean is set or width/height set to 1.\n",
        "if not TILE_IMAGES or (TILES_WIDE == 1 and TILES_HIGH == 1):\n",
        "  TILES_WIDE = 1\n",
        "  TILES_HIGH = 1\n",
        "  TILE_IMAGES = False\n",
        "  \n",
        "#@markdown **Prompt(s) for tiles**\n",
        "\n",
        "#@markdown **Global tile prompt** uses PROMPT (previous cell) for *all* tiles (e.g. \"Roman mosaic of an unswept floor\")\n",
        "GLOBAL_TILE_PROMPT = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Otherwise, specify multiple tile prompts with columns separated by | and / to delineate new row.\n",
        "\n",
        "\n",
        "#@markdown E.g. multiple prompts for a 3x2 \"landscape\" image : \"sun | clouds | sky / fields | fields | trees\"\n",
        "\n",
        "TILE_PROMPT_STRING = \"sun | clouds / colorful buildings | colourful buildings\"   #@param {type:\"string\"}\n",
        "\n",
        "if not TILE_IMAGES or GLOBAL_TILE_PROMPT:\n",
        "  TILE_PROMPTS = [GLOBAL_PROMPT] * TILES_HIGH * TILES_WIDE\n",
        "else:\n",
        "  TILE_PROMPTS = []\n",
        "  count_y = 0\n",
        "  count_x = 0\n",
        "  for row in TILE_PROMPT_STRING.split(\"/\"):\n",
        "    for prompt in row.split(\"|\"):\n",
        "      prompt = prompt.strip()\n",
        "      TILE_PROMPTS.append(prompt)\n",
        "      count_x += 1\n",
        "    if count_x != TILES_WIDE:\n",
        "      raise ValueError(f\"Insufficient prompts for row {count_y}; expected {TILES_WIDE} but got {count_x}\")\n",
        "    count_x = 0\n",
        "    count_y += 1\n",
        "  if count_y != TILES_HIGH:\n",
        "    raise ValueError(f\"Insufficient prompt rows; expected {TILES_HIGH} but got {count_y}\")\n",
        "\n",
        "print(\"Tile prompts: \", TILE_PROMPTS)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QFvC6js6LVuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Composition prompts (within tiles)\n",
        "\n",
        "#@markdown **Single image** (i.e. no tiling) composition prompts\n",
        "\n",
        "#@markdown Specify 3x3 prompts for each composition region (left to right, starting at the top)\n",
        "PROMPT_x0_y0 = \"a photorealistic sky with sun\"   #@param {type:\"string\"}\n",
        "PROMPT_x1_y0 = \"a photorealistic sky\"   #@param {type:\"string\"}\n",
        "PROMPT_x2_y0 = \"a photorealistic sky with moon\"   #@param {type:\"string\"}\n",
        "PROMPT_x0_y1 = \"a photorealistic tree\"   #@param {type:\"string\"}\n",
        "PROMPT_x1_y1 = \"a photorealistic tree\"   #@param {type:\"string\"}\n",
        "PROMPT_x2_y1 = \"a photorealistic tree\"   #@param {type:\"string\"}\n",
        "PROMPT_x0_y2 = \"a photorealistic field\"   #@param {type:\"string\"}\n",
        "PROMPT_x1_y2 = \"a photorealistic field\"   #@param {type:\"string\"}\n",
        "PROMPT_x2_y2 = \"a photorealistic chicken\"   #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Tile** composition prompts\n",
        "\n",
        "#@markdown This string is formated to autogenerate region prompts from tile prompt. e.g. \"close-up of {}\"\n",
        "TILE_PROMPT_FORMATING = \"close-up of {}\"  #@param {type:\"string\"}\n",
        "\n",
        "tile_count = 0\n",
        "all_prompts = []\n",
        "for y in range(TILES_HIGH):\n",
        "  for x in range(TILES_WIDE):\n",
        "    tile_prompts = []\n",
        "    if COMPOSITIONAL_IMAGE:\n",
        "      if TILE_IMAGES:\n",
        "        tile_prompts = [\n",
        "            TILE_PROMPT_FORMATING.format(TILE_PROMPTS[tile_count])] * 9\n",
        "      else:\n",
        "        tile_prompts = [PROMPT_x0_y0, PROMPT_x1_y0, PROMPT_x2_y0, \n",
        "                        PROMPT_x0_y1, PROMPT_x1_y1, PROMPT_x2_y1, \n",
        "                        PROMPT_x0_y2, PROMPT_x1_y2, PROMPT_x2_y2]\n",
        "    tile_prompts.append(TILE_PROMPTS[tile_count])\n",
        "    tile_count += 1\n",
        "    all_prompts.append(tile_prompts)\n",
        "print(f\"All prompts: {all_prompts}\")\n",
        "\n",
        "# Example prompt lists for different settings, where\n",
        "# PROMPT = \"Roman\"\n",
        "# TILE_PROMPT_FORMATING = \"close-up of {}\"\n",
        "# TILE_PROMPT_STRING = \"sun | clouds | sky / fields | fields | trees\"\n",
        "\n",
        "# 1. Single image with **global** prompt\n",
        "#   * Tile 0 prompts: ['Roman']\n",
        "# 1. Single image with **composition** prompts (tested)\n",
        "#   * Tile 0 prompts: ['close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'Roman']\n",
        "# 1. Tiled images with **global** prompt for each tile.\n",
        "#   * Tile 0 prompts: ['Roman']\n",
        "#   * Tile 1 prompts: ['Roman']\n",
        "#   * Tile 2 prompts: ['Roman']\n",
        "#   * Tile 3 prompts: ['Roman']\n",
        "#   * Tile 4 prompts: ['Roman']\n",
        "#   * Tile 5 prompts: ['Roman']\n",
        "# 1. Tiled images with **global** prompt for each tile.\n",
        "#   * Tile 0 prompts: ['sun']\n",
        "#   * Tile 1 prompts: ['clouds']\n",
        "#   * Tile 2 prompts: ['sky']\n",
        "#   * Tile 3 prompts: ['fields']\n",
        "#   * Tile 4 prompts: ['fields']\n",
        "#   * Tile 5 prompts: ['trees']\n",
        "# 1. Tiled images with separate **composition** prompts for each tile.\n",
        "#   * Tile 0 prompts: ['close-up of sun', 'close-up of sun', 'close-up of sun', 'close-up of sun', 'close-up of sun', 'close-up of sun', 'close-up of sun', 'close-up of sun', 'close-up of sun', 'sun']\n",
        "#   * Tile 1 prompts: ['close-up of clouds', 'close-up of clouds', 'close-up of clouds', 'close-up of clouds', 'close-up of clouds', 'close-up of clouds', 'close-up of clouds', 'close-up of clouds', 'close-up of clouds', 'clouds']\n",
        "#   * Tile 2 prompts: ['close-up of sky', 'close-up of sky', 'close-up of sky', 'close-up of sky', 'close-up of sky', 'close-up of sky', 'close-up of sky', 'close-up of sky', 'close-up of sky', 'sky']\n",
        "#   * Tile 3 prompts: ['close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'fields']\n",
        "#   * Tile 4 prompts: ['close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'close-up of fields', 'fields']\n",
        "#   * Tile 5 prompts: ['close-up of trees', 'close-up of trees', 'close-up of trees', 'close-up of trees', 'close-up of trees', 'close-up of trees', 'close-up of trees', 'close-up of trees', 'close-up of trees', 'trees']\n",
        "# [188]\n",
        "# \n",
        "# 1. Tiled images with **global** **composition** prompts for each tile.\n",
        "#   * Tile 0 prompts: ['close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'Roman']\n",
        "#   * Tile 1 prompts: ['close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'Roman']\n",
        "#   * Tile 2 prompts: ['close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'Roman']\n",
        "#   * Tile 3 prompts: ['close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'Roman']\n",
        "#   * Tile 4 prompts: ['close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'Roman']\n",
        "#   * Tile 5 prompts: ['close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'close-up of Roman', 'Roman']"
      ],
      "metadata": {
        "id": "Iz10BbmO4w5n",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create collage! (Initialisation)\n",
        "ct = CollageTiler(wide=TILES_WIDE,\n",
        "                  high=TILES_HIGH,\n",
        "                  prompts=all_prompts,\n",
        "                  segmented_data=segmented_data, \n",
        "                  segmented_data_high_res=segmented_data_high_res, \n",
        "                  fixed_background_image=background_image,\n",
        "                  background_use=BACKGROUND_USE,\n",
        "                  compositional=COMPOSITIONAL_IMAGE,\n",
        "                  high_res_multiplier=MULTIPLIER_BIG_IMAGE,\n",
        "                  output_dir='collage_tiler_out',\n",
        "                  file_basename='testing_tiler',\n",
        "                  video_steps=0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QKfQHdXtvspC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Collage (Loop)\n",
        "#@markdown To edit patches interrupt this cell and run the one below this. Re-run this cell afterwards to continue generating the image.\n",
        "output = ct.loop()"
      ],
      "metadata": {
        "id": "Y6QyfhZ5VBsJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tinker with patches\n",
        "#@markdown Enable this cell to allow patch editing:\n",
        "PATCH_TINKERING = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Interupt the cell above mid-optimisation and run this cell to manually adjust the patches. Run it several times to adjust different patches. Then re-run the cell above to continue optimising.\n",
        "\n",
        "if PATCH_TINKERING:\n",
        "  from ipywidgets import interactive\n",
        "  import IPython.display\n",
        "  from google.colab.output import eval_js\n",
        "  import base64\n",
        "  \n",
        "  # Render the current collage(s).\n",
        "  generator = collage_maker.generator\n",
        "  step = collage_maker.step\n",
        "  params = {'gamma': step / OPTIM_STEPS}\n",
        "  img = generator(params)\n",
        "  img = img.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "  print('Current collage(s)')\n",
        "  res_img = show_and_save(img, t=step,\n",
        "                          max_display=MAX_MULTIPLE_VISUALISATIONS,\n",
        "                          stitch=True, show=False)\n",
        "  filename_temp = f\"./temp.png\"\n",
        "  res_img = cv2.cvtColor(res_img, cv2.COLOR_BGR2RGB) * 255\n",
        "  cv2.imwrite(filename_temp, res_img)\n",
        "  \n",
        "  # HTML code to plot the image and detect the mouse cursor.\n",
        "  canvas_html = \"\"\"\n",
        "  <canvas width=%d height=%d></canvas>\n",
        "  <script>\n",
        "    var filename_image = \"%s\"\n",
        "    var canvas = document.querySelector('canvas')\n",
        "    var ctx = canvas.getContext('2d')\n",
        "    ctx.lineWidth = 1\n",
        "    var mouse = {x: 0, y: 0}\n",
        "    canvas.addEventListener('mousemove', function(e) {\n",
        "      mouse.x = e.pageX - this.offsetLeft\n",
        "      mouse.y = e.pageY - this.offsetTop\n",
        "    })\n",
        "    canvas.onmousedown = ()=>{\n",
        "      ctx.beginPath()\n",
        "      ctx.moveTo(mouse.x, mouse.y)\n",
        "      canvas.addEventListener('mousemove', onPaint)\n",
        "    }\n",
        "    var onPaint = ()=>{\n",
        "      ctx.lineTo(mouse.x, mouse.y)\n",
        "      ctx.stroke()\n",
        "    }\n",
        "    var data = new Promise(resolve=>{\n",
        "      canvas.onmouseup = ()=>{\n",
        "        canvas.removeEventListener('mousemove', onPaint)\n",
        "        resolve(mouse)\n",
        "      }\n",
        "    })\n",
        "    function draw_collage_image() {\n",
        "      collage_image = new Image();\n",
        "      collage_image.src = filename_image;\n",
        "      collage_image.onload = function(){\n",
        "        ctx.drawImage(collage_image, 0, 0);\n",
        "      }\n",
        "    }\n",
        "    draw_collage_image();\n",
        "  </script>\n",
        "  \"\"\"\n",
        "  \n",
        "  im = IPython.display.Image(filename_temp, embed=True)\n",
        "  # IPython.display.display(im)\n",
        "  filename_embed = 'data:image/png;base64,'\n",
        "  filename_embed += base64.b64encode(im.data).decode('ascii')\n",
        "  \n",
        "  # Display an HTML canvas with the image.\n",
        "  canvas = IPython.display.HTML(\n",
        "      canvas_html % (CANVAS_WIDTH * POP_SIZE, CANVAS_HEIGHT, filename_embed))\n",
        "  print('Click with the mouse on the desired image and patch:')\n",
        "  IPython.display.display(canvas)\n",
        "  \n",
        "  # Select the image and pixel coordinates.\n",
        "  def draw():\n",
        "    print('draw()')\n",
        "    mouse = eval_js('data')\n",
        "    return mouse\n",
        "  mouse = draw()\n",
        "  pop_id_mouse = int(np.floor(mouse['x'] / CANVAS_WIDTH))\n",
        "  x_mouse = int(mouse['x'] % CANVAS_WIDTH)\n",
        "  y_mouse = int(mouse['y'])\n",
        "  print(f'Selected image {pop_id_mouse} at ({x_mouse}, {y_mouse})')\n",
        "  \n",
        "  def find_patch(generator, id, u, v):\n",
        "    # Render only the spatial transforms of the patches.\n",
        "    rendered_patches = generator.spatial_transformer(generator.patches)\n",
        "    rendered_patches = rendered_patches.detach().cpu().numpy()\n",
        "    patch_id = np.argmax(rendered_patches[id, :, 3, u, v] * rendered_patches[id, :, 4, u, v])\n",
        "    return patch_id\n",
        "  \n",
        "  # Select the patch.\n",
        "  patch_id = find_patch(generator, pop_id_mouse, y_mouse, x_mouse)\n",
        "  print(f'Found matching patch {patch_id}')\n",
        "  \n",
        "  # Extract the patch's current affine transform paramaters.\n",
        "  with torch.no_grad():\n",
        "    x0 = generator.spatial_transformer.translation[pop_id_mouse, patch_id, 0, 0]\n",
        "    x0 = float(x0.detach().cpu().numpy())\n",
        "    y0 = generator.spatial_transformer.translation[pop_id_mouse, patch_id, 1, 0]\n",
        "    y0 = float(y0.detach().cpu().numpy())\n",
        "    rot0 = generator.spatial_transformer.rotation[pop_id_mouse, patch_id, 0, 0]\n",
        "    rot0 = float(rot0.detach().cpu().numpy())\n",
        "    scale0 = generator.spatial_transformer.scale[pop_id_mouse, patch_id, 0, 0]\n",
        "    scale0 = float(scale0.detach().cpu().numpy())\n",
        "    squeeze0 = generator.spatial_transformer.squeeze[pop_id_mouse, patch_id, 0, 0]\n",
        "    squeeze0 = float(squeeze0.detach().cpu().numpy())\n",
        "    shear0 = generator.spatial_transformer.shear[pop_id_mouse, patch_id, 0, 0]\n",
        "    shear0 = float(shear0.detach().cpu().numpy())\n",
        "    patch_info = {'pop_id': pop_id_mouse, 'patch_id': patch_id,\n",
        "                  'x0': x0, 'y0': y0, 'rot0': rot0,\n",
        "                  'scale0': scale0, 'squeeze0': squeeze0, 'shear0': shear0,\n",
        "                  'x': x0, 'y': y0, 'rot': rot0,\n",
        "                  'scale': scale0, 'squeeze': squeeze0, 'shear': shear0}\n",
        "  \n",
        "  def show_modified(dx, dy, drot, dscale, dsqueeze, dshear):\n",
        "    \"\"\"Visualization callback function with affine transform deltas.\"\"\"\n",
        "    with torch.no_grad():\n",
        "      x = patch_info['x0'] - dx\n",
        "      y = patch_info['y0'] + dy\n",
        "      rot = patch_info['rot0'] - drot\n",
        "      scale = patch_info['scale0'] - dscale\n",
        "      squeeze = patch_info['squeeze0'] + dsqueeze\n",
        "      shear = patch_info['shear0'] + dshear\n",
        "      generator.spatial_transformer.translation[pop_id_mouse, patch_id, 0, 0] = x\n",
        "      generator.spatial_transformer.translation[pop_id_mouse, patch_id, 1, 0] = y\n",
        "      generator.spatial_transformer.rotation[pop_id_mouse, patch_id, 0, 0] = rot\n",
        "      generator.spatial_transformer.scale[pop_id_mouse, patch_id, 0, 0] = scale\n",
        "      generator.spatial_transformer.squeeze[pop_id_mouse, patch_id, 0, 0] = squeeze\n",
        "      generator.spatial_transformer.shear[pop_id_mouse, patch_id, 0, 0] = shear\n",
        "    patch_info['x'] = x\n",
        "    patch_info['y'] = y\n",
        "    patch_info['rot'] = rot\n",
        "    patch_info['shear'] = shear\n",
        "    patch_info['squeeze'] = squeeze\n",
        "    patch_info['shear'] = shear\n",
        "    params = {'gamma': step / OPTIM_STEPS}\n",
        "    img = generator(params)\n",
        "    img = img.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "    _ = show_and_save(img, t=step,\n",
        "                      max_display=MAX_MULTIPLE_VISUALISATIONS,\n",
        "                      stitch=True)\n",
        "  \n",
        "  # Interactive editing of the patch's affine transform parameters.\n",
        "  interactive_plot = interactive(show_modified,\n",
        "                                dx=(-MAX_TRANS * 2, MAX_TRANS * 2, 0.01),\n",
        "                                dy=(-MAX_TRANS * 2, MAX_TRANS * 2, 0.01),\n",
        "                                drot=(-MAX_ROT * 2, MAX_ROT * 2, 0.01),\n",
        "                                dscale=(-MAX_SCALE * 2, MAX_SCALE * 2, 0.01),\n",
        "                                dsqueeze=(-MAX_SQUEEZE * 2, MAX_SQUEEZE * 2, 0.01),\n",
        "                                dshear=(-MAX_SHEAR * 2, MAX_SHEAR * 2, 0.01))\n",
        "  output = interactive_plot.children[-1]\n",
        "  output.layout.height = '350px'\n",
        "else:\n",
        "  interactive_plot = \"Patch tinkering not enabled.\"\n",
        "interactive_plot\n",
        "  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "OZpKu3XcdogG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Render high res image and finish up.\n",
        "\n",
        "ct.assemble_tiles()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "F4ley6lsEtPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save and download assets (WIP) \n",
        "\n",
        "raise ValueError(\"Stop here.\")\n",
        "\n",
        "OUTPUT_DIR = f\"{FILE_BASENAME}_results\"\n",
        "!mkdir {OUTPUT_DIR}\n",
        "!rm {OUTPUT_DIR}/*\n",
        "\n",
        "# Prepare the outputs.\n",
        "zipname = f\"{FILE_BASENAME}_rendering.zip\"\n",
        "print(f\"Output to {OUTPUT_DIR}, zipfile will be saved to {zipname}\")\n",
        "\n",
        "!zip -r {zipname} {OUTPUT_DIR}\n",
        "from google.colab import files\n",
        "files.download(zipname)\n"
      ],
      "metadata": {
        "id": "YVst6OhPVx_s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9biyGTC-6DOy"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G264zsJ0XV-3"
      },
      "outputs": [],
      "source": [
        "raise ValueError(\"Stop here.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1lQ8EKwObOD5"
      },
      "outputs": [],
      "source": [
        "#@title zip up outputs and download\n",
        "!zip -r {zipname} {OUTPUT_DIR}\n",
        "from google.colab import files\n",
        "files.download(zipname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c144I7cg6Hrh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Upload the image patches\n",
        "#@markdown Run this cell to upload a npy file containing segmented patches.\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  with open(fn, 'rb') as f:\n",
        "    segmented_data_initial = np.load(f, allow_pickle = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjYI9mBDHSxm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Creates patch file from PNGs\n",
        "#@markdown Colab needs to be changed to support local npy files.\n",
        "\n",
        "import imageio\n",
        "import glob\n",
        "\n",
        "TARGET_FILE = \"/content/patches.npy\"\n",
        "PNG_DIR = \"/content/pngs\"\n",
        "mkdir(PNG_DIR)\n",
        "\n",
        "def upload_files(target_path):\n",
        "  \"\"\"Upload files to target directory.\"\"\"\n",
        "  mkdir(target_path)\n",
        "  uploaded = files.upload()\n",
        "  for k, v in uploaded.items():\n",
        "    open(target_path + \"/\" + k, 'wb').write(v)\n",
        "  return list(uploaded.keys())\n",
        "\n",
        "png_imgs = []\n",
        "for png_im_path in glob.glob(PNG_DIR + \"/*.png\"):\n",
        "     png_im = imageio.imread(png_im_path)\n",
        "     print(png_im.shape)\n",
        "     png_imgs.append(png_im)\n",
        "\n",
        "png_imgs_np = np.array(png_imgs)\n",
        "np.save(TARGET_FILE, png_imgs_np, allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu0jhLcGHz_6"
      },
      "source": [
        "#Scratch Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XleAKCQ850e_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Render image 0 and 1 with each rendering method\n",
        "\n",
        "rms = [\"opacity\", \"transparency\", \"masked_transparency_clipped\", \"masked_transparency_normed\"]\n",
        "for rm in rms:\n",
        "  RENDER_METHOD = rm\n",
        "  print(rm)\n",
        "  t = 1\n",
        "  params = {'gamma': t}\n",
        "  img = generator(params)\n",
        "  img = img.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "  show_and_save(img, t=t, max_display=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure tiling\n",
        "# TILES_WIDTH = 1  #@param {type:\"slider\", min:1, max:8}\n",
        "# TILES_HEIGHT = 1  #@param {type:\"slider\", min:1, max:8}\n",
        "# TILES_PROMPT = \"\"  #@param {type:\"string\"}\n",
        "# \n",
        "# if not TILES_PROMPT\n",
        "#   TILES_PROMPT = PROMPT\n",
        "# \n",
        "# tiled_prompts = []\n",
        "# for x in range(TILES_WIDTH):\n",
        "#   for y in range(TILES_HEIGHT):\n",
        "#     tiled_prompts.append(PROMPTS)\n",
        "# PROMPTS = tiled_prompts"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Aa96frytD3t8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BfEKdIbK4VZa",
        "c8bdUyJs4hq3",
        "bTWV2a7ZekET",
        "HxR0ZFpAebsH",
        "r9GsjxMcgAP7",
        "ubwSr59fgk0C",
        "4ckYmVuAAO7x",
        "GIXzueO3PB-4",
        "IpPJx-r_QZq3",
        "9biyGTC-6DOy",
        "Mu0jhLcGHz_6"
      ],
      "machine_shape": "hm",
      "name": "Arnheim_3_Collage_1_26.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}